{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "188ef8ec",
   "metadata": {},
   "source": [
    "#### Test Classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1041cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "classifier = pipeline(\"zero-shot-classification\",\n",
    "                      model=\"joeddav/xlm-roberta-large-xnli\",\n",
    "                      use_fast=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0308e68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "clause_1 = \"\"\"c) ƒê·∫ßu t∆∞ ph√°t tri·ªÉn VLXD tr√™n ƒë·ªãa b√†n t·ªânh theo nhu c·∫ßu c·ªßa th·ªã tr∆∞·ªùng v√† c√°c quy ho·∫°ch, ƒë·ªÅ √°n, k·∫ø ho·∫°ch ƒë∆∞·ª£c duy·ªát; kh√¥ng ƒë·∫ßu t∆∞ c√°c d·ª± √°n s·∫£n xu·∫•t VLXD ·ªü c√°c v√πng ·∫£nh h∆∞·ªüng ƒë·∫øn h√†nh lang b·∫£o v·ªá c√¥ng tr√¨nh qu·ªëc ph√≤ng, an ninh, giao th√¥ng, thu·ª∑ l·ª£i, ƒë√™ ƒëi·ªÅu, nƒÉng l∆∞·ª£ng, khu di t√≠ch, l·ªãch s·ª≠ - vƒÉn h√≥a v√† khu v·ª±c b·∫£o v·ªá c√¥ng tr√¨nh kh√°c theo quy ƒë·ªãnh c·ªßa ph√°p lu·∫≠t.\"\"\"\n",
    "\n",
    "clause_2 = \"\"\"g) C√°c d·ª± √°n s·∫£n xu·∫•t v·∫≠t li·ªáu x√¢y d·ª±ng tr√™n ƒë·ªãa b√†n t·ªânh v·∫´n ƒë∆∞·ª£c ph√©p ƒë·∫ßu t∆∞ t·∫°i c√°c khu v·ª±c n·∫±m trong h√†nh lang b·∫£o v·ªá c√¥ng tr√¨nh qu·ªëc ph√≤ng, an ninh, giao th√¥ng, thu·ª∑ l·ª£i, ƒë√™ ƒëi·ªÅu, nƒÉng l∆∞·ª£ng, di t√≠ch l·ªãch s·ª≠ ‚Äì vƒÉn h√≥a v√† c√°c khu v·ª±c b·∫£o v·ªá c√¥ng tr√¨nh kh√°c n·∫øu c√≥ nhu c·∫ßu ph√°t tri·ªÉn th·ªã tr∆∞·ªùng v√† ph√π h·ª£p v·ªõi quy ho·∫°ch.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a4183b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "model_name = \"MoritzLaurer/mDeBERTa-v3-base-xnli-multilingual-nli-2mil7\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "model.to(device)\n",
    "\n",
    "premise = clause_1\n",
    "hypothesis = clause_2\n",
    "\n",
    "inputs = tokenizer(premise, hypothesis, truncation=True, return_tensors=\"pt\")\n",
    "inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "output = model(**inputs)\n",
    "prediction = torch.softmax(output[\"logits\"][0], -1).tolist()\n",
    "label_names = [\"entailment\", \"neutral\", \"contradiction\"]\n",
    "prediction = {name: round(float(pred) * 100, 1) for pred, name in zip(prediction, label_names)}\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0caa492c",
   "metadata": {},
   "outputs": [],
   "source": [
    "premise = clause_1\n",
    "hypothesis = clause_2\n",
    "\n",
    "inputs = tokenizer(premise, hypothesis, truncation=True, return_tensors=\"pt\")\n",
    "inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "output = model(**inputs)\n",
    "prediction = torch.softmax(output[\"logits\"][0], -1).tolist()\n",
    "label_names = [\"entailment\", \"neutral\", \"contradiction\"]\n",
    "prediction = {name: round(float(pred) * 100, 1) for pred, name in zip(prediction, label_names)}\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba6da454",
   "metadata": {},
   "source": [
    "#### Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba000673",
   "metadata": {},
   "outputs": [],
   "source": [
    "clause_1 = \"\"\"c) ƒê·∫ßu t∆∞ ph√°t tri·ªÉn VLXD tr√™n ƒë·ªãa b√†n t·ªânh theo nhu c·∫ßu c·ªßa th·ªã tr∆∞·ªùng v√† c√°c quy ho·∫°ch, ƒë·ªÅ √°n, k·∫ø ho·∫°ch ƒë∆∞·ª£c duy·ªát; kh√¥ng ƒë·∫ßu t∆∞ c√°c d·ª± √°n s·∫£n xu·∫•t VLXD ·ªü c√°c v√πng ·∫£nh h∆∞·ªüng ƒë·∫øn h√†nh lang b·∫£o v·ªá c√¥ng tr√¨nh qu·ªëc ph√≤ng, an ninh, giao th√¥ng, thu·ª∑ l·ª£i, ƒë√™ ƒëi·ªÅu, nƒÉng l∆∞·ª£ng, khu di t√≠ch, l·ªãch s·ª≠ - vƒÉn h√≥a v√† khu v·ª±c b·∫£o v·ªá c√¥ng tr√¨nh kh√°c theo quy ƒë·ªãnh c·ªßa ph√°p lu·∫≠t.\"\"\"\n",
    "\n",
    "clause_2 = \"\"\"g) C√°c d·ª± √°n s·∫£n xu·∫•t v·∫≠t li·ªáu x√¢y d·ª±ng tr√™n ƒë·ªãa b√†n t·ªânh v·∫´n ƒë∆∞·ª£c ph√©p ƒë·∫ßu t∆∞ t·∫°i c√°c khu v·ª±c n·∫±m trong h√†nh lang b·∫£o v·ªá c√¥ng tr√¨nh qu·ªëc ph√≤ng, an ninh, giao th√¥ng, thu·ª∑ l·ª£i, ƒë√™ ƒëi·ªÅu, nƒÉng l∆∞·ª£ng, di t√≠ch l·ªãch s·ª≠ ‚Äì vƒÉn h√≥a v√† c√°c khu v·ª±c b·∫£o v·ªá c√¥ng tr√¨nh kh√°c n·∫øu c√≥ nhu c·∫ßu ph√°t tri·ªÉn th·ªã tr∆∞·ªùng v√† ph√π h·ª£p v·ªõi quy ho·∫°ch.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1140a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "from pyvi import ViTokenizer\n",
    "\n",
    "# Download from the ü§ó Hub\n",
    "model = SentenceTransformer(\"huyydangg/DEk21_hcmute_embedding\")\n",
    "\n",
    "# Define query (c√¢u h·ªèi ph√°p lu·∫≠t) v√† docs (ƒëi·ªÅu lu·∫≠t)\n",
    "query = clause_1\n",
    "docs = [clause_2]\n",
    "\n",
    "# T√°ch t·ª´ cho query\n",
    "segmented_query = ViTokenizer.tokenize(query)\n",
    "\n",
    "# T√°ch t·ª´ cho t·ª´ng d√≤ng vƒÉn b·∫£n\n",
    "segmented_docs = [ViTokenizer.tokenize(doc) for doc in docs]\n",
    "\n",
    "# Encode query and documents\n",
    "query_embedding = model.encode([segmented_query])\n",
    "doc_embeddings = model.encode(segmented_docs)\n",
    "similarities = torch.nn.functional.cosine_similarity(\n",
    "    torch.tensor(query_embedding), torch.tensor(doc_embeddings)\n",
    ").flatten()\n",
    "\n",
    "# Sort documents by cosine similarity\n",
    "sorted_indices = torch.argsort(similarities, descending=True)\n",
    "sorted_docs = [docs[idx] for idx in sorted_indices]\n",
    "sorted_scores = [similarities[idx].item() for idx in sorted_indices]\n",
    "\n",
    "# Print sorted documents with their cosine scores\n",
    "for doc, score in zip(sorted_docs, sorted_scores):\n",
    "    print(f\"Document: {doc} - Cosine Similarity: {score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ccf4e72",
   "metadata": {},
   "source": [
    "#### Get document\n",
    "\n",
    "1. Extract clause area\n",
    "2. Using cosine similarity\n",
    "3. Using \"MoritzLaurer/mDeBERTa-v3-base-xnli-multilingual-nli-2mil7\" model in three labels: \"entailment\", \"neutral\", \"contradiction\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc37b7b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import docx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from level_detector import LevelDetector\n",
    "\n",
    "level_detector = LevelDetector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723038c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_path = \"/home/ducmb/fis_legal_nlp/F1/test.docx\"\n",
    "\n",
    "paragraphs = docx.Document(temp_path).paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c24155a",
   "metadata": {},
   "outputs": [],
   "source": [
    "begin = 0\n",
    "n = len(paragraphs)\n",
    "clauses = []\n",
    "sample_stop = ['m·∫´u b√°o c√°o', 'm·∫´u bi√™n b·∫£n', 'm·∫´u c√¥ng vƒÉn', 'm·∫´u gi·∫•y gi·ªõi thi·ªáu', 'm·∫´u k·∫ø ho·∫°ch', 'm·∫´u quy·∫øt ƒë·ªãnh c√° bi·ªát ban h√†nh vƒÉn b·∫£n k√®m theo']\n",
    "\n",
    "while begin < n:\n",
    "    heading = paragraphs[begin].text\n",
    "    level = level_detector.get_level(heading)\n",
    "\n",
    "    if level['level_id'] == 0 and any(stop in heading.lower() for stop in sample_stop):\n",
    "        break\n",
    "\n",
    "    if level['type'] != 'ƒëi·ªÅu':\n",
    "        begin += 1\n",
    "        continue\n",
    "\n",
    "    current_entry = [heading]\n",
    "    clauses.append(current_entry)   \n",
    "\n",
    "    next_idx = begin + 1\n",
    "    while next_idx < n:\n",
    "        body_text = paragraphs[next_idx].text\n",
    "        if body_text == '':\n",
    "            next_idx += 1\n",
    "            continue\n",
    "        body_level = level_detector.get_level(body_text)\n",
    "\n",
    "        if body_level['level_id'] < 3 or body_level['type'] == 'ƒëi·ªÅu' or body_text in [' ', '\\xa0']:\n",
    "            break\n",
    "\n",
    "        current_entry.append(body_text)\n",
    "        next_idx += 1\n",
    "\n",
    "\n",
    "    begin = next_idx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34e6fd8",
   "metadata": {},
   "source": [
    "#### Test chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a62d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from FisReader.document_factory import DocumentFactory\n",
    "from ChunkExtractor.chunk_extractor import ChunkExtractor\n",
    "import pathlib\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = pathlib.Path(\"/home/maibaduc/nlp/LegalChunk/data/02_2011_QD_KTNN_m_127602.docx\")\n",
    "doc_id = \"1\"\n",
    "\n",
    "document = DocumentFactory(ocr_endpoint=\"http://10.15.84.44:6999\").read(file_path, doc_id, for_llm=True)\n",
    "with open(\"tree_test.json\", \"w\", encoding='utf8') as f:\n",
    "    json.dump(document.tree_structure, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "\n",
    "chunk_extractor = ChunkExtractor()\n",
    "\n",
    "chunks = chunk_extractor.get_chunks_in_tree(document, doc_id, file_path, \"data.docx\")\n",
    "\n",
    "print(type(chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e2896c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk in chunks:\n",
    "    print(chunk)\n",
    "    print(\"*\"*100)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47c60b2",
   "metadata": {},
   "source": [
    "#### End of chunking test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5862c776",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "from pyvi import ViTokenizer\n",
    "\n",
    "embed_model = SentenceTransformer(\"huyydangg/DEk21_hcmute_embedding\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec39674",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_article_structure(article_groups):\n",
    "    \"\"\"\n",
    "    Extract hierarchical legal references (Article ‚Üí Clause ‚Üí Point)\n",
    "    from pre-parsed document segments.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    article_groups : list[list[str]]\n",
    "        A list where each element represents one article block.\n",
    "        Each article block contains a list of raw segments (ƒêi·ªÅu, Kho·∫£n, ƒêi·ªÉm, ...).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list[list[str]]\n",
    "        A nested list where each article group is mapped to a list of\n",
    "        fully-qualified legal references, such as:\n",
    "            - \"ƒêi·ªÅu 12\"\n",
    "            - \"Kho·∫£n 2 ƒêi·ªÅu 12\"\n",
    "            - \"ƒêi·ªÉm b Kho·∫£n 2 ƒêi·ªÅu 12\"\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    - If a clause contains points, only the points will be added (not the clause itself).\n",
    "    - If a clause has no points, the clause will be added to the output.\n",
    "    - The function respects the legal hierarchy: Article > Clause > Point.\n",
    "    \"\"\"\n",
    "\n",
    "    final_results = []\n",
    "\n",
    "    for group in article_groups:\n",
    "        output = []\n",
    "\n",
    "        if len(group) == 1:\n",
    "            output.append(group[0])\n",
    "        elif len(group) == 2:\n",
    "            article_text = group[0]\n",
    "            clause_text = group[1]\n",
    "            output.append(f\"{clause_text} {article_text}\")\n",
    "        # Case: complex structure: ƒêi·ªÅu ‚Äì Kho·∫£n ‚Äì ƒêi·ªÉm\n",
    "        else:\n",
    "            article = \"\"\n",
    "            clause = \"\"\n",
    "            point = \"\"\n",
    "            sub_point = \"\"\n",
    "            output = []\n",
    "\n",
    "            for index, doc in enumerate(group):\n",
    "                curr_level = level_detector.get_level(doc)\n",
    "                curr_level_type = curr_level[\"type\"]\n",
    "                curr_level_id = curr_level[\"level_id\"]\n",
    "\n",
    "                # L·∫•y level c·ªßa ph·∫ßn t·ª≠ ti·∫øp theo (ho·∫∑c 0 n·∫øu l√† ph·∫ßn t·ª≠ cu·ªëi)\n",
    "                if index + 1 < len(group):\n",
    "                    next_level = level_detector.get_level(group[index + 1])\n",
    "                    next_level_id = next_level[\"level_id\"]\n",
    "                else:\n",
    "                    next_level_id = 0  # sentinel: coi nh∆∞ h·∫øt vƒÉn b·∫£n ‚Üí bu·ªôc flush\n",
    "\n",
    "                # ------------------ ƒêI·ªÄU ------------------\n",
    "                if curr_level_type == \"ƒëi·ªÅu\":\n",
    "                    # Khi g·∫∑p ƒêi·ªÅu m·ªõi ‚Üí reset context th·∫•p h∆°n\n",
    "                    article = doc\n",
    "                    clause = \"\"\n",
    "                    point = \"\"\n",
    "                    sub_point = \"\"\n",
    "\n",
    "                    # N·∫øu ƒêi·ªÅu n√†y kh√¥ng c√≥ Kho·∫£n/ƒêi·ªÉm ph√≠a sau\n",
    "                    # (v√≠ d·ª•: ƒêi·ªÅu X., sau ƒë√≥ l√† ƒêi·ªÅu Y. ho·∫∑c h·∫øt lu√¥n)\n",
    "                    if curr_level_id >= next_level_id:\n",
    "                        output.append(article)\n",
    "\n",
    "                # ------------------ KHO·∫¢N ------------------\n",
    "                elif curr_level_type == \"kho·∫£n\":\n",
    "                    clause = \"Kho·∫£n \" + doc\n",
    "                    point = \"\"\n",
    "                    sub_point = \"\"\n",
    "\n",
    "                    # N·∫øu sau Kho·∫£n kh√¥ng c√≤n c·∫•p th·∫•p h∆°n (ƒëi·ªÉm/ƒëi·ªÉm con/n·ªôi dung)\n",
    "                    # m√† nh·∫£y sang Kho·∫£n kh√°c, ƒêi·ªÅu kh√°c ho·∫∑c h·∫øt vƒÉn b·∫£n\n",
    "                    if curr_level_id >= next_level_id:\n",
    "                        parts = [clause, article]  # \"1. Kho·∫£n ƒêi·ªÅu 1.\"\n",
    "                        output.append(\" \".join(parts))\n",
    "\n",
    "                # ------------------ ƒêI·ªÇM ------------------\n",
    "                elif curr_level_type == \"ƒëi·ªÉm\":\n",
    "                    point = \"ƒêi·ªÉm \" + doc\n",
    "                    sub_point = \"\"\n",
    "\n",
    "                    if curr_level_id >= next_level_id:\n",
    "                        parts = [point]\n",
    "                        if clause:\n",
    "                            parts.append(clause)\n",
    "                        if article:\n",
    "                            parts.append(article)\n",
    "                        output.append(\" \".join(parts))\n",
    "\n",
    "                # ------------------ ƒêI·ªÇM CON ------------------\n",
    "                elif curr_level_type == \"ƒëi·ªÉm con\":\n",
    "                    sub_point = doc\n",
    "\n",
    "                    if curr_level_id >= next_level_id:\n",
    "                        parts = [sub_point]\n",
    "                        if point:\n",
    "                            parts.append(point)\n",
    "                        if clause:\n",
    "                            parts.append(clause)\n",
    "                        if article:\n",
    "                            parts.append(article)\n",
    "                        output.append(\" \".join(parts))\n",
    "\n",
    "                # ------------------ N·ªòI DUNG ------------------\n",
    "                elif curr_level_type == \"n·ªôi dung\":\n",
    "                    # n·ªôi dung chi ti·∫øt n·∫±m d∆∞·ªõi c√πng\n",
    "                    if curr_level_id >= next_level_id:\n",
    "                        parts = [doc]\n",
    "                        if sub_point:\n",
    "                            parts.append(sub_point)\n",
    "                        if point:\n",
    "                            parts.append(point)\n",
    "                        if clause:\n",
    "                            parts.append(clause)\n",
    "                        if article:\n",
    "                            parts.append(article)\n",
    "                        output.append(\" \".join(parts))\n",
    "\n",
    "        final_results.append(output)\n",
    "\n",
    "    return final_results\n",
    "\n",
    "# Example usage:\n",
    "article_hierarchy = extract_article_structure(clauses)\n",
    "article_hierarchy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a44024b",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_article_hierarchy = []\n",
    "\n",
    "for docs in article_hierarchy:\n",
    "    segmented_docs = [ViTokenizer.tokenize(doc) for doc in docs]\n",
    "    embed_docs = embed_model.encode(segmented_docs)\n",
    "    embed_article_hierarchy.append(embed_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2cc98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "pairs = list(combinations(range(len(article_hierarchy)), 2))\n",
    "print(pairs)\n",
    "print(len(pairs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d17ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_pairs(article_hierarchy, embed_article_hierarchy, pairs):\n",
    "    result = {}\n",
    "    for pair in pairs:\n",
    "        ind_0, ind_1 = pair[0], pair[1]\n",
    "        embed_1 = embed_article_hierarchy[ind_0]\n",
    "        embed_2 = embed_article_hierarchy[ind_1]\n",
    "        for i in range(len(embed_1)):\n",
    "            similarities = torch.nn.functional.cosine_similarity(\n",
    "                torch.tensor(embed_1[i]), torch.tensor(embed_2)\n",
    "            ).flatten()\n",
    "            temp_index = [index for index, value in enumerate(similarities) if value > 0.4]\n",
    "            \n",
    "            if len(temp_index) > 0:\n",
    "                key = article_hierarchy[ind_0][i]\n",
    "                values = [article_hierarchy[ind_1][index] for index in temp_index]\n",
    "\n",
    "                if key not in result:\n",
    "                    result[key] = values\n",
    "                else:\n",
    "                    result[key].extend(values)\n",
    "    return result\n",
    "\n",
    "best_article_pairs = get_best_pairs(article_hierarchy, embed_article_hierarchy, pairs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0321f81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = \"d) Lu·∫≠t n√†y kh√¥ng ƒëi·ªÅu ch·ªânh ho·∫°t ƒë·ªông nghi√™n c·ª©u v√† ph√°t tri·ªÉn h·ªá th·ªëng tr√≠ tu·ªá nh√¢n t·∫°o.\"\n",
    "\n",
    "keys = list(best_article_pairs.keys())\n",
    "\n",
    "for key, values in best_article_pairs.items():\n",
    "    if key == temp:\n",
    "        print(values)\n",
    "    for value in values:\n",
    "        if value == temp:\n",
    "            print(key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64fe06d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "keys_pairs = list(best_article_pairs.keys())\n",
    "print(keys_pairs[0])\n",
    "print(len(list(best_article_pairs.values())))\n",
    "best_article_pairs[keys_pairs[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6bd652d",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_1 = embed_article_hierarchy[0]\n",
    "embed_2 = embed_article_hierarchy[3]\n",
    "\n",
    "print(embed_1.shape)\n",
    "print(embed_2.shape)\n",
    "\n",
    "similarities = torch.nn.functional.cosine_similarity(\n",
    "    torch.tensor(embed_1[0]), torch.tensor(embed_2)\n",
    ").flatten()\n",
    "\n",
    "temp_index = [index for index, value in enumerate(similarities) if value > 0.5]\n",
    "print(temp_index)\n",
    "\n",
    "if len(temp_index) == 0:\n",
    "    print(\"No similar pairs found\")\n",
    "\n",
    "sorted_indices = torch.argsort(similarities, descending=True)\n",
    "\n",
    "print(sorted_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9377ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "clause_1 = \"\"\n",
    "clause_2 = \"\"\n",
    "\n",
    "for i, (k, v) in enumerate(result.items()):\n",
    "    if i == 1:\n",
    "        clause_1 = k\n",
    "        clause_2 = v[3]\n",
    "    if i > 1:\n",
    "        break\n",
    "\n",
    "print(clause_1)\n",
    "print(clause_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3089962e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "model_name = \"MoritzLaurer/mDeBERTa-v3-base-xnli-multilingual-nli-2mil7\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b0723c",
   "metadata": {},
   "outputs": [],
   "source": [
    "premise = \"\"\"ƒêi·ªÅu 1. Ban h√†nh k√®m theo Quy·∫øt ƒë·ªãnh n√†y Quy ƒë·ªãnh ni√™m phong t√†i li·ªáu, ki·ªÉm tra t√†i kho·∫£n c·ªßa ƒë∆°n v·ªã ƒë∆∞·ª£c ki·ªÉm to√°n v√† c√° nh√¢n c√≥ li√™n quan trong ho·∫°t ƒë·ªông ki·ªÉm to√°n c·ªßa Ki·ªÉm to√°n Nh√† n∆∞·ªõc.\"\"\"\n",
    "hypothesis = \"\"\"Kho·∫£n 1. Th·ª±c hi·ªán c√°c quy·ªÅn v√† tr√°ch nhi·ªám quy ƒë·ªãnh t·∫°i ƒêi·ªÅu 22 c·ªßa Quy ƒë·ªãnh n√†y. ƒêi·ªÅu 23. Quy·ªÅn v√† tr√°ch nhi·ªám c·ªßa ƒë∆°n v·ªã ƒë∆∞·ª£c ki·ªÉm to√°n v√† c√° nh√¢n c√≥ t√†i kho·∫£n b·ªã ki·ªÉm tra\"\"\"\n",
    "\n",
    "inputs = tokenizer(premise, hypothesis, truncation=True, return_tensors=\"pt\")\n",
    "inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "output = model(**inputs)\n",
    "prediction = torch.softmax(output[\"logits\"][0], -1).tolist()\n",
    "label_names = [\"entailment\", \"neutral\", \"contradiction\"]\n",
    "prediction = {name: round(float(pred) * 100, 1) for pred, name in zip(prediction, label_names)}\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f03ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_contract_pair(close_pairs):\n",
    "    result = []\n",
    "\n",
    "    for key, values in close_pairs.items():\n",
    "        for value in values:\n",
    "            premise = key\n",
    "            hypothesis = value\n",
    "            inputs = tokenizer(premise, hypothesis, truncation=True, return_tensors=\"pt\")\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            output = model(**inputs)\n",
    "            prediction = torch.softmax(output[\"logits\"][0], -1).tolist()\n",
    "            label_names = [\"entailment\", \"neutral\", \"contradiction\"]\n",
    "            prediction = {name: round(float(pred) * 100, 1) for pred, name in zip(prediction, label_names)}\n",
    "            if prediction['contradiction'] > prediction['entailment'] and prediction['contradiction'] > prediction['neutral']:\n",
    "                result.append((premise, hypothesis))\n",
    "    return result\n",
    "\n",
    "\n",
    "result_1 = get_contract_pair(result)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601748d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from pydantic import BaseModel\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=\"sk-F79270ATXBohPIr56I4jLA\",\n",
    "    base_url=\"https://mkp-api.fptcloud.com/v1\",\n",
    ")\n",
    "\n",
    "class ConflictResult(BaseModel):\n",
    "    score: float              # numeric score 0 ‚Üí 1\n",
    "    interpretation: str       # detailed explanation\n",
    "\n",
    "\n",
    "prompt_system = (\n",
    "    \"You are a senior legal NLP analyst specialized in Vietnamese regulatory documents. \"\n",
    "    \"Your task is to evaluate the relationship between two legal clauses \"\n",
    "    \"and quantify the degree of internal conflict or contradiction between them. \"\n",
    "    \"Return:\\n\"\n",
    "    \"1) a numerical conflict score between 0.0 and 1.0, where 0.0 means no conflict and 1.0 means full contradiction,\\n\"\n",
    "    \"2) a concise interpretation in Vietnamese explaining the rationale of the score.\\n\"\n",
    "    \"Do not provide legal advice. Stay objective, analytical, and deterministic.\"\n",
    ")\n",
    "\n",
    "def analyze_conflict(clause_a: str, clause_b: str):\n",
    "\n",
    "    response = client.responses.parse(\n",
    "        model=\"gpt-oss-20b\",\n",
    "        input=[\n",
    "            {\"role\": \"system\", \"content\": prompt_system},\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": (\n",
    "                    \"Clause A:\\n\"\n",
    "                    f\"{clause_a}\\n\\n\"\n",
    "                    \"Clause B:\\n\"\n",
    "                    f\"{clause_b}\\n\\n\"\n",
    "                    \"Analyze and score their level of internal conflict.\"\n",
    "                ),\n",
    "            },\n",
    "        ],\n",
    "        text_format=ConflictResult,\n",
    "    )\n",
    "\n",
    "    return response.output_parsed\n",
    "\n",
    "\n",
    "# -------------------\n",
    "# Example Usage\n",
    "# -------------------\n",
    "\n",
    "clause_a = contract_pairs[0][0]\n",
    "clause_b = contract_pairs[0][1]\n",
    "\n",
    "result = analyze_conflict(clause_a, clause_b)\n",
    "\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f586e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(best_article_pairs.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4c27ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "conflict_pairs = []\n",
    "for key, values in best_article_pairs.items():\n",
    "    for value in values:\n",
    "        result = analyze_conflict(key, value)\n",
    "        score =  result.__dict__['score']\n",
    "        if score > 0.5:\n",
    "            conflict_pairs.append((key, value, result.__dict__['interpretation']))\n",
    "\n",
    "conflict_pairs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404408ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel\n",
    "\n",
    "model = AutoModel.from_pretrained(\n",
    "    'jinaai/jina-reranker-v3',\n",
    "    dtype=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189f6f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_1 = \"c) ƒê·∫ßu t∆∞ ph√°t tri·ªÉn VLXD tr√™n ƒë·ªãa b√†n t·ªânh theo nhu c·∫ßu c·ªßa th·ªã tr∆∞·ªùng v√† c√°c quy ho·∫°ch, ƒë·ªÅ √°n, k·∫ø ho·∫°ch ƒë∆∞·ª£c duy·ªát; kh√¥ng ƒë·∫ßu t∆∞ c√°c d·ª± √°n s·∫£n xu·∫•t VLXD ·ªü c√°c v√πng ·∫£nh h∆∞·ªüng ƒë·∫øn h√†nh lang b·∫£o v·ªá c√¥ng tr√¨nh qu·ªëc ph√≤ng, an ninh, giao th√¥ng, thu·ª∑ l·ª£i, ƒë√™ ƒëi·ªÅu, nƒÉng l∆∞·ª£ng, khu di t√≠ch, l·ªãch s·ª≠ - vƒÉn h√≥a v√† khu v·ª±c b·∫£o v·ªá c√¥ng tr√¨nh kh√°c theo quy ƒë·ªãnh c·ªßa ph√°p lu·∫≠t.\"\n",
    "\n",
    "test_2 = \"m) Trong tr∆∞·ªùng h·ª£p c√≥ y√™u c·∫ßu th√∫c ƒë·∫©y tƒÉng tr∆∞·ªüng c√¥ng nghi·ªáp ƒë·ªãa ph∆∞∆°ng, c√°c d·ª± √°n s·∫£n xu·∫•t v·∫≠t li·ªáu x√¢y d·ª±ng c√≥ th·ªÉ xem x√©t tri·ªÉn khai t·∫°i nh·ªØng khu v·ª±c thu·ªôc h√†nh lang b·∫£o v·ªá c√¥ng tr√¨nh qu·ªëc ph√≤ng, an ninh, giao th√¥ng, th·ªßy l·ª£i, ƒë√™ ƒëi·ªÅu, nƒÉng l∆∞·ª£ng, di t√≠ch l·ªãch s·ª≠ ‚Äì vƒÉn h√≥a v√† m·ªôt s·ªë khu v·ª±c b·∫£o v·ªá c√¥ng tr√¨nh kh√°c theo ƒë√°nh gi√° c·ªßa c∆° quan c√≥ th·∫©m quy·ªÅn.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "168a6e51",
   "metadata": {},
   "source": [
    "c) ƒê·∫ßu t∆∞ ph√°t tri·ªÉn VLXD tr√™n ƒë·ªãa b√†n t·ªânh theo nhu c·∫ßu c·ªßa th·ªã tr∆∞·ªùng v√† c√°c quy ho·∫°ch, ƒë·ªÅ √°n, k·∫ø ho·∫°ch ƒë∆∞·ª£c duy·ªát; kh√¥ng ƒë·∫ßu t∆∞ c√°c d·ª± √°n s·∫£n xu·∫•t VLXD ·ªü c√°c v√πng ·∫£nh h∆∞·ªüng ƒë·∫øn h√†nh lang b·∫£o v·ªá c√¥ng tr√¨nh qu·ªëc ph√≤ng, an ninh, giao th√¥ng, thu·ª∑ l·ª£i, ƒë√™ ƒëi·ªÅu, nƒÉng l∆∞·ª£ng, khu di t√≠ch, l·ªãch s·ª≠ - vƒÉn h√≥a v√† khu v·ª±c b·∫£o v·ªá c√¥ng tr√¨nh kh√°c theo quy ƒë·ªãnh c·ªßa ph√°p lu·∫≠t."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae71eb5d",
   "metadata": {},
   "source": [
    "m) Trong tr∆∞·ªùng h·ª£p c√≥ y√™u c·∫ßu th√∫c ƒë·∫©y tƒÉng tr∆∞·ªüng c√¥ng nghi·ªáp ƒë·ªãa ph∆∞∆°ng, c√°c d·ª± √°n s·∫£n xu·∫•t v·∫≠t li·ªáu x√¢y d·ª±ng c√≥ th·ªÉ xem x√©t tri·ªÉn khai t·∫°i nh·ªØng khu v·ª±c thu·ªôc h√†nh lang b·∫£o v·ªá c√¥ng tr√¨nh qu·ªëc ph√≤ng, an ninh, giao th√¥ng, th·ªßy l·ª£i, ƒë√™ ƒëi·ªÅu, nƒÉng l∆∞·ª£ng, di t√≠ch l·ªãch s·ª≠ ‚Äì vƒÉn h√≥a v√† m·ªôt s·ªë khu v·ª±c b·∫£o v·ªá c√¥ng tr√¨nh kh√°c theo ƒë√°nh gi√° c·ªßa c∆° quan c√≥ th·∫©m quy·ªÅn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e78190e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_index = 0\n",
    "\n",
    "query = keys_pairs[test_index]\n",
    "\n",
    "documents = best_article_pairs[keys_pairs[test_index]]\n",
    "\n",
    "# Rerank documents\n",
    "results = model.rerank(query, documents)\n",
    "\n",
    "# Results are sorted by relevance score (highest first)\n",
    "for result in results:\n",
    "    print(f\"Score: {result['relevance_score']:.4f}\")\n",
    "    print(f\"Document: {result['document'][:100]}...\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11420324",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_contract_pair(close_pairs):\n",
    "    contract_pair = []\n",
    "\n",
    "    for index, (key, values) in enumerate(close_pairs.items()):\n",
    "        query = key\n",
    "        documents = values\n",
    "        print(f'Index: {index}')\n",
    "        results = model.rerank(query, documents)\n",
    "        for result in results:\n",
    "            if result['relevance_score'] >  0.5:\n",
    "                contract_pair.append((query, result['document']))\n",
    "    return contract_pair\n",
    "\n",
    "\n",
    "contract_pairs = get_contract_pair(best_article_pairs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc4762f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "\n",
    "def rerank_one(args):\n",
    "    key, docs = args\n",
    "    results = model.rerank(key, docs)\n",
    "    return [(key, r[\"document\"]) for r in results if r[\"relevance_score\"] > 0.5]\n",
    "\n",
    "def get_contract_pair(close_pairs):\n",
    "    with Pool(processes=8) as p:\n",
    "        results = p.map(rerank_one, close_pairs.items())\n",
    "    contract_pairs = [item for sub in results for item in sub]\n",
    "    return contract_pairs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581322f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = [(1, 2), (3, 4)] \n",
    "\n",
    "for k, v in temp:\n",
    "    print(k,v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b97e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "contract_pairs = get_contract_pair(best_article_pairs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7074f722",
   "metadata": {},
   "source": [
    "#### Test on ContractNLI Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8f3c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8baacd5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "pipe = pipeline(\"text-classification\",model=\"tasksource/deberta-base-long-nli\")\n",
    "out = pipe([dict(text='there is a cat', text_pair='there is a black cat'), \n",
    "dict(text=\"there is a cat\", text_pair=\"there is a black cat\")]) #list of (premise,hypothesis)\n",
    "# [{'label': 'neutral', 'score': 0.9952911138534546}] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c860731c",
   "metadata": {},
   "outputs": [],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc97d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "\n",
    "model_name = \"VietAI/envit5-translation\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to('cuda')\n",
    "\n",
    "inputs = [\n",
    "    \"vi: Lu·∫≠t n√†y kh√¥ng ƒëi·ªÅu ch·ªânh ho·∫°t ƒë·ªông nghi√™n c·ª©u v√† ph√°t tri·ªÉn h·ªá th·ªëng tr√≠ tu·ªá nh√¢n t·∫°o.\",\n",
    "    \"vi: Lu·∫≠t n√†y ƒëi·ªÅu ch·ªânh c√°c ho·∫°t ƒë·ªông cung c·∫•p, tri·ªÉn khai, s·ª≠ d·ª•ng h·ªá th·ªëng tr√≠ tu·ªá nh√¢n t·∫°o v√† quy ƒë·ªãnh quy·ªÅn, nghƒ©a v·ª• c·ªßa t·ªï ch·ª©c, c√° nh√¢n tham gia ho·∫°t ƒë·ªông tr√≠ tu·ªá nh√¢n t·∫°o t·∫°i Vi·ªát Nam.\",\n",
    "    \"vi: T·ªï ch·ª©c, c√° nh√¢n ƒë∆∞·ª£c quy·ªÅn t·ª± do kinh doanh m·ªçi ng√†nh, ngh·ªÅ m√† ph√°p lu·∫≠t kh√¥ng c·∫•m; ƒë·ªìng th·ªùi ƒë∆∞·ª£c quy·ªÅn m·ªü r·ªông quy m√¥ s·∫£n xu·∫•t ‚Äì kinh doanh m√† kh√¥ng b·ªã gi·ªõi h·∫°n v·ªÅ s·ªë l∆∞·ª£ng lao ƒë·ªông, ph·∫°m vi ho·∫°t ƒë·ªông v√† kh·ªëi l∆∞·ª£ng s·∫£n ph·∫©m, tr·ª´ tr∆∞·ªùng h·ª£p c√°c ho·∫°t ƒë·ªông ƒë√≥ g√¢y ·∫£nh h∆∞·ªüng nghi√™m tr·ªçng ƒë·∫øn qu·ªëc ph√≤ng, an ninh qu·ªëc gia ho·∫∑c m√¥i tr∆∞·ªùng sinh th√°i.\",\n",
    "    \"vi: M·ªçi t·ªï ch·ª©c, c√° nh√¢n trong n∆∞·ªõc ch·ªâ ƒë∆∞·ª£c ph√©p ti·∫øn h√†nh ho·∫°t ƒë·ªông s·∫£n xu·∫•t ‚Äì kinh doanh trong ph·∫°m vi ƒë·ªãa b√†n c·∫•p t·ªânh n∆°i ƒëƒÉng k√Ω kinh doanh ban ƒë·∫ßu, v√† tuy·ªát ƒë·ªëi kh√¥ng ƒë∆∞·ª£c m·ªü r·ªông quy m√¥ nh√¢n s·ª±, c∆° s·ªü v·∫≠n h√†nh ho·∫∑c tƒÉng kh·ªëi l∆∞·ª£ng s·∫£n ph·∫©m n·∫øu kh√¥ng c√≥ s·ª± ch·∫•p thu·∫≠n b·∫±ng vƒÉn b·∫£n c·ªßa c∆° quan qu·∫£n l√Ω chuy√™n ng√†nh c·∫•p trung ∆∞∆°ng.\"]\n",
    "\n",
    "outputs = model.generate(tokenizer(inputs, return_tensors=\"pt\", padding=True).input_ids.to('cuda'), max_length=512)\n",
    "translated_sentences = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "translated_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80beb2f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(chunks):\n",
    "    model_name = \"VietAI/envit5-translation\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to('cuda')\n",
    "\n",
    "    translated_sentences = []       \n",
    "    for sentences in chunks:\n",
    "        inputs = [f'vi: {sentence}' for sentence in sentences]\n",
    "        outputs = model.generate(tokenizer(inputs, return_tensors=\"pt\", padding=True).input_ids.to('cuda'), max_length=512)\n",
    "        outputs = [output[4:] for output in tokenizer.batch_decode(outputs, skip_special_tokens=True)]\n",
    "        translated_sentences.append(outputs)\n",
    "    return translated_sentences\n",
    "\n",
    "translated_sentences = translate(article_hierarchy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09bb8815",
   "metadata": {},
   "outputs": [],
   "source": [
    "for paragraph in paragraphs:\n",
    "    print(paragraph.text)\n",
    "    print(level_detector.get_level(paragraph.text))\n",
    "    print(\"-\"*100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232247fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "group = [\n",
    "    'ƒêi·ªÅu 1.',\n",
    "    '1. Kho·∫£n 1',\n",
    "    'a) ƒêi·ªÉm c',\n",
    "    'b) ƒêi·ªÉm b',\n",
    "    '',\n",
    "    'd) ƒêi·ªÉm d',\n",
    "    '2. Kho·∫£n 2',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86186b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "article = \"\"\n",
    "clause = \"\"\n",
    "point = \"\"\n",
    "sub_point = \"\"\n",
    "output = []\n",
    "\n",
    "for segment_idx, text_segment in enumerate(group):\n",
    "    curr_level = level_detector.get_level(text_segment)\n",
    "    curr_level_type = curr_level[\"type\"]\n",
    "    curr_level_id = curr_level[\"level_id\"]\n",
    "\n",
    "    # Look ahead to the next element's level (or 0 if this is the last one)\n",
    "    if segment_idx + 1 < len(group):\n",
    "        next_level = level_detector.get_level(group[segment_idx + 1])\n",
    "        next_level_id = next_level[\"level_id\"]\n",
    "    else:\n",
    "        next_level_id = 0  # sentinel: end of group ‚Üí force flush\n",
    "\n",
    "    # ------------------ ARTICLE (ƒêI·ªÄU) ------------------\n",
    "    if curr_level_type == \"ƒëi·ªÅu\":\n",
    "        # Reset lower-level context when a new article is found\n",
    "        article = text_segment\n",
    "        clause = \"\"\n",
    "        point = \"\"\n",
    "        sub_point = \"\"\n",
    "\n",
    "        # If there is no lower-level element after this article,\n",
    "        # we add the article itself.\n",
    "        if curr_level_id >= next_level_id:\n",
    "            output.append(article)\n",
    "\n",
    "    # ------------------ CLAUSE (KHO·∫¢N) ------------------\n",
    "    elif curr_level_type == \"kho·∫£n\":\n",
    "        clause = \"Kho·∫£n \" + text_segment\n",
    "        point = \"\"\n",
    "        sub_point = \"\"\n",
    "\n",
    "        # If no lower-level element follows (point/sub-point/content),\n",
    "        # we add the clause together with its article.\n",
    "        if curr_level_id >= next_level_id:\n",
    "            parts = [clause]\n",
    "            if article:\n",
    "                parts.append(article)\n",
    "            output.append(\" \".join(parts))\n",
    "\n",
    "    # ------------------ POINT (ƒêI·ªÇM) ------------------\n",
    "    elif curr_level_type == \"ƒëi·ªÉm\":\n",
    "        point = \"ƒêi·ªÉm \" + text_segment\n",
    "        sub_point = \"\"\n",
    "\n",
    "        if curr_level_id >= next_level_id:\n",
    "            parts = [point]\n",
    "            if clause:\n",
    "                parts.append(clause)\n",
    "            if article:\n",
    "                parts.append(article)\n",
    "            output.append(\" \".join(parts))\n",
    "\n",
    "    # ------------------ SUB-POINT (ƒêI·ªÇM CON) ------------------\n",
    "    elif curr_level_type == \"ƒëi·ªÉm con\":\n",
    "        sub_point = text_segment\n",
    "\n",
    "        if curr_level_id >= next_level_id:\n",
    "            parts = [sub_point]\n",
    "            if point:\n",
    "                parts.append(point)\n",
    "            if clause:\n",
    "                parts.append(clause)\n",
    "            if article:\n",
    "                parts.append(article)\n",
    "            output.append(\" \".join(parts))\n",
    "\n",
    "    # ------------------ CONTENT (N·ªòI DUNG) ------------------\n",
    "    elif curr_level_type == \"n·ªôi dung\":\n",
    "        # Lowest-level detailed content\n",
    "        if curr_level_id >= next_level_id:\n",
    "            parts = [text_segment]\n",
    "            if sub_point:\n",
    "                parts.append(sub_point)\n",
    "            if point:\n",
    "                parts.append(point)\n",
    "            if clause:\n",
    "                parts.append(clause)\n",
    "            if article:\n",
    "                parts.append(article)\n",
    "            output.append(\" \".join(parts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba1d9dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f4ecbac",
   "metadata": {},
   "outputs": [],
   "source": [
    "for articles in article_hierarchy:\n",
    "    for article in articles:\n",
    "        if \"d) Lu·∫≠t n√†y kh√¥ng ƒëi·ªÅu ch·ªânh ho·∫°t ƒë·ªông nghi√™n c·ª©u v√† ph√°t tri·ªÉn h·ªá th·ªëng tr√≠ tu·ªá nh√¢n t·∫°o.\" in article:\n",
    "            print(articles)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126de859",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"BatsResearch/bonito-experiment-eval\", \"contract_nli\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad44059c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "preds = []\n",
    "labels = []\n",
    "\n",
    "for row in tqdm(ds[\"test\"]):\n",
    "    prem = row[\"premise\"]\n",
    "    hypo = row[\"hypothesis\"]\n",
    "    out = pipe({\"text\": prem, \"text_pair\": hypo})\n",
    "\n",
    "    preds.append(out[\"label\"])\n",
    "    labels.append(row[\"label\"])\n",
    "\n",
    "map_label_to_int = {\"entailment\": 1, \"neutral\": 2, \"contradiction\": 3}\n",
    "pred_ids = [map_label_to_int[x] for x in preds]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b2bdc8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "acc = accuracy_score(labels, pred_ids)\n",
    "print(\"Accuracy =\", acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7fb556",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_path = \"/home/ducmb/fis_legal_nlp/F1/test.docx\"\n",
    "\n",
    "paragraphs_1 = docx.Document(temp_path).paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d87cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "translated_sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08ff4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = {}\n",
    "\n",
    "for pair in pairs:\n",
    "    article_1 = translated_sentences[pair[0]]\n",
    "    article_2 = translated_sentences[pair[1]]\n",
    "\n",
    "    for i in range(len(article_1)):\n",
    "        temp_1 = article_1[i]\n",
    "        pipe_input = []\n",
    "        for j in range(len(article_2)):\n",
    "            temp_2 = article_2[j]\n",
    "            pipe_input.append(dict(text=temp_1, text_pair=temp_2))\n",
    "        outs = pipe(pipe_input)\n",
    "        for index, out in enumerate(outs):\n",
    "            if out[\"label\"] == \"contradict\":\n",
    "                if output[temp_1] is None:\n",
    "                    output[temp_1] = article_2[index]\n",
    "                else:\n",
    "                    output[temp_1].append(article_2[index])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93b7248",
   "metadata": {},
   "outputs": [],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ada0a5b",
   "metadata": {},
   "source": [
    "#### Using LLM to extract sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a19edf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 2 files:   0%|          | 0/2 [12:59<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"Qwen/Qwen2.5-3B-Instruct\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "prompt = \"Give me a short introduction to large language model.\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "generated_ids = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=512\n",
    ")\n",
    "generated_ids = [\n",
    "    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "]\n",
    "\n",
    "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c78a259",
   "metadata": {},
   "source": [
    "#### Extract information in sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7b19a93c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-26 10:25:26 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.11.0.json: 435kB [00:00, 6.46MB/s]                    \n",
      "2025-11-26 10:25:26 INFO: Downloaded file to /home/ducmb/stanza_resources/resources.json\n",
      "2025-11-26 10:25:27 INFO: Loading these models for language: vi (Vietnamese):\n",
      "================================\n",
      "| Processor    | Package       |\n",
      "--------------------------------\n",
      "| tokenize     | vtb           |\n",
      "| pos          | vtb_charlm    |\n",
      "| constituency | vlsp22_charlm |\n",
      "================================\n",
      "\n",
      "2025-11-26 10:25:27 INFO: Using device: cuda\n",
      "2025-11-26 10:25:27 INFO: Loading: tokenize\n",
      "2025-11-26 10:25:27 INFO: Loading: pos\n",
      "2025-11-26 10:25:28 INFO: Loading: constituency\n",
      "2025-11-26 10:25:28 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(ROOT (S (NP (N Vi·ªác) (SBAR (S (NP (Det nh·ªØng) (N c√¥ng ngh·ªá) (Adj m·ªõi)) (VP (VP (Adj li√™n t·ª•c) (V xu·∫•t hi·ªán)) (CC v√†) (VP (V t·∫°o) (Adv ra) (NP (Det nh·ªØng) (N thay ƒë·ªïi) (AP (Adj kh√≥) (V l∆∞·ªùng)) (PP (Pre trong) (NP (N ƒë·ªùi s·ªëng) (N x√£ h·ªôi))))))))) (VP (V ƒë·∫∑t) (Adv ra) (NP (V y√™u c·∫ßu) (SBAR (SC r·∫±ng) (S (S (NP (Det c√°c) (N c∆° quan) (V qu·∫£n l√Ω)) (, ,) (SC d√π) (VP (AUX ph·∫£i) (VP (V ƒë·ªëi m·∫∑t) (PP (Pre v·ªõi) (NP (Adj nhi·ªÅu) (N h·∫°n ch·∫ø) (PP (Pre v·ªÅ) (NP (N ngu·ªìn l·ª±c)))))))) (, ,) (VP (Adv v·∫´n) (V c·∫ßn) (VP (V x√¢y d·ª±ng) (NP (Det nh·ªØng) (N khung) (N ph√°p l√Ω) (AP (Adj ƒë·ªß) (Adj linh ho·∫°t)) (PP (Pre ƒë·ªÉ) (VP (V b·∫£o ƒë·∫£m) (NP (N quy·ªÅn) (CC v√†) (N l·ª£i √≠ch) (AP (Adj ch√≠nh ƒë√°ng)) (PP (Pre c·ªßa) (NP (N ng∆∞·ªùi) (N d√¢n)))))))))))))))\n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "\n",
    "nlp = stanza.Pipeline(lang='vi', processors='tokenize,pos,constituency')\n",
    "doc = nlp('Vi·ªác nh·ªØng c√¥ng ngh·ªá m·ªõi li√™n t·ª•c xu·∫•t hi·ªán v√† t·∫°o ra nh·ªØng thay ƒë·ªïi kh√≥ l∆∞·ªùng trong ƒë·ªùi s·ªëng x√£ h·ªôi ƒë·∫∑t ra y√™u c·∫ßu r·∫±ng c√°c c∆° quan qu·∫£n l√Ω, d√π ph·∫£i ƒë·ªëi m·∫∑t v·ªõi nhi·ªÅu h·∫°n ch·∫ø v·ªÅ ngu·ªìn l·ª±c, v·∫´n c·∫ßn x√¢y d·ª±ng nh·ªØng khung ph√°p l√Ω ƒë·ªß linh ho·∫°t ƒë·ªÉ b·∫£o ƒë·∫£m quy·ªÅn v√† l·ª£i √≠ch ch√≠nh ƒë√°ng c·ªßa ng∆∞·ªùi d√¢n')\n",
    "for sentence in doc.sentences:\n",
    "    print(sentence.constituency)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "395c3ffb",
   "metadata": {},
   "source": [
    "(ROOT (S (NP (N Vi·ªác) (SBAR (S (NP (Det nh·ªØng) (N c√¥ng ngh·ªá) (Adj m·ªõi)) (VP (VP (Adj li√™n t·ª•c) (V xu·∫•t hi·ªán)) (CC v√†) (VP (V t·∫°o) (Adv ra) (NP (Det nh·ªØng) (N thay ƒë·ªïi) (AP (Adj kh√≥) (V l∆∞·ªùng)) (PP (Pre trong) (NP (N ƒë·ªùi s·ªëng) (N x√£ h·ªôi))))))))) (VP (V ƒë·∫∑t) (Adv ra) (NP (V y√™u c·∫ßu) (SBAR (SC r·∫±ng) (S (S (NP (Det c√°c) (N c∆° quan) (V qu·∫£n l√Ω)) (, ,) (SC d√π) (VP (AUX ph·∫£i) (VP (V ƒë·ªëi m·∫∑t) (PP (Pre v·ªõi) (NP (Adj nhi·ªÅu) (N h·∫°n ch·∫ø) (PP (Pre v·ªÅ) (NP (N ngu·ªìn l·ª±c)))))))) (, ,) (VP (Adv v·∫´n) (V c·∫ßn) (VP (V x√¢y d·ª±ng) (NP (Det nh·ªØng) (N khung) (N ph√°p l√Ω) (AP (Adj ƒë·ªß) (Adj linh ho·∫°t)) (PP (Pre ƒë·ªÉ) (VP (V b·∫£o ƒë·∫£m) (NP (N quy·ªÅn) (CC v√†) (N l·ª£i √≠ch) (AP (Adj ch√≠nh ƒë√°ng)) (PP (Pre c·ªßa) (NP (N ng∆∞·ªùi) (N d√¢n)))))))))))))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f0a8b02e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(ROOT (S (S (NP (N Tr∆∞·ªùng h·ª£p)) (VP (Adv ƒë√£) (V c√≥) (NP (N s·ªë) (VP (V ƒë·ªãnh danh) (NP (N c√° nh√¢n)))))) (, ,) (S (NP (N ng∆∞·ªùi) (VP (V th·ª±c hi·ªán) (NP (N th·ªß t·ª•c) (VP (V ƒëƒÉng k√Ω) (NP (N doanh nghi·ªáp)))))) (VP (VP (V k√™ khai) (NP (Det c√°c) (N th√¥ng tin) (PP (Pre v·ªÅ) (NP (NP (N h·ªç)) (, ,) (NP (N ch·ªØ) (V ƒë·ªám)) (CC v√†) (NP (N t√™n)) (, ,) (NP (N ng√†y)) (, ,) (NP (N th√°ng)) (, ,) (NP (N nƒÉm) (N sinh)) (, ,) (NP (N s·ªë) (VP (V ƒë·ªãnh danh) (NP (N c√° nh√¢n)))) (, ,) (NP (N gi·ªõi t√≠nh)) (PP (Pre c·ªßa) (NP (N m√¨nh))) (CC v√†) (PP (Pre c·ªßa) (NP (N c√° nh√¢n))))))) (VP (V c√≥) (NP (V y√™u c·∫ßu) (VP (V k√™ khai) (NP (N th√¥ng tin) (Adj c√° nh√¢n) (PP (Pre trong) (NP (N h·ªì s∆°) (VP (V ƒëƒÉng k√Ω) (NP (N doanh nghi·ªáp))))))))) (CC v√†) (VP (V ƒë·ªìng √Ω) (VP (V chia s·∫ª) (NP (N th√¥ng tin) (Adj c√° nh√¢n) (VP (AUX ƒë∆∞·ª£c) (SBAR (S (VP (V l∆∞u gi·ªØ) (PP (Pre t·∫°i) (NP (N C∆° s·ªü d·ªØ li·ªáu) (N qu·ªëc gia) (PP (Pre v·ªÅ) (NP (N d√¢n c∆∞))))) (PP (Pre cho) (NP (NP (N C∆° quan) (VP (V ƒëƒÉng k√Ω) (V kinh doanh))) (, ,) (NP (N c∆° quan) (VP (V qu·∫£n l√Ω) (NP (N nh√† n∆∞·ªõc)))) (PP (Pre v·ªÅ) (VP (V ƒëƒÉng k√Ω) (V kinh doanh) (PP (Pre ƒë·ªÉ) (VP (V ph·ª•c v·ª•) (NP (N c√¥ng t√°c) (VP (V qu·∫£n l√Ω) (NP (N nh√† n∆∞·ªõc))) (PP (Pre v·ªÅ) (VP (V ƒëƒÉng k√Ω) (NP (N doanh nghi·ªáp)) (VP (Pre theo) (VP (N quy ƒë·ªãnh))))))))))))))))))))) (. .)))\n",
      "(ROOT (S (NP (Det C√°c) (N th√¥ng tin) (V k√™ khai)) (VP (AUX ƒë∆∞·ª£c) (VP (V ƒë·ªëi chi·∫øu) (PP (Pre v·ªõi) (NP (Det c√°c) (N th√¥ng tin) (VP (AUX ƒë∆∞·ª£c) (VP (V l∆∞u gi·ªØ) (PP (Pre t·∫°i) (NP (N C∆° s·ªü d·ªØ li·ªáu) (N qu·ªëc gia) (PP (Pre v·ªÅ) (NP (. d√¢n c∆∞.)))))))))))))\n",
      "(ROOT (S (S (NP (N Tr∆∞·ªùng h·ª£p) (SBAR (S (VP (N th√¥ng tin) (NP (N c√° nh√¢n))) (VP (AUX ƒë∆∞·ª£c) (SBAR (S (VP (V k√™ khai)))))))) (VP (Adv kh√¥ng) (Adj th·ªëng nh·∫•t) (VP (V so) (PP (Pre v·ªõi) (NP (N th√¥ng tin) (VP (AUX ƒë∆∞·ª£c) (SBAR (S (VP (V l∆∞u gi·ªØ) (PP (Pre t·∫°i) (NP (N C∆° s·ªü d·ªØ li·ªáu) (N qu·ªëc gia) (PP (Pre v·ªÅ) (NP (N d√¢n c∆∞)))))))))))))) (, ,) (S (NP (N c√° nh√¢n) (Pro ƒë√≥)) (VP (V c√≥) (NP (N tr√°ch nhi·ªám) (VP (V c·∫≠p nh·∫≠t) (, ,) (V ƒëi·ªÅu ch·ªânh) (NP (N th√¥ng tin)) (PP (Pre ƒë·ªÉ) (VP (V ƒë·∫£m b·∫£o) (AP (Adj ch√≠nh x√°c)) (, ,) (Adj th·ªëng nh·∫•t))))))) (. .)))\n"
     ]
    }
   ],
   "source": [
    "doc = nlp('Tr∆∞·ªùng h·ª£p ƒë√£ c√≥ s·ªë ƒë·ªãnh danh c√° nh√¢n, ng∆∞·ªùi th·ª±c hi·ªán th·ªß t·ª•c ƒëƒÉng k√Ω doanh nghi·ªáp k√™ khai c√°c th√¥ng tin v·ªÅ h·ªç, ch·ªØ ƒë·ªám v√† t√™n, ng√†y, th√°ng, nƒÉm sinh, s·ªë ƒë·ªãnh danh c√° nh√¢n, gi·ªõi t√≠nh c·ªßa m√¨nh v√† c·ªßa c√° nh√¢n c√≥ y√™u c·∫ßu k√™ khai th√¥ng tin c√° nh√¢n trong h·ªì s∆° ƒëƒÉng k√Ω doanh nghi·ªáp v√† ƒë·ªìng √Ω chia s·∫ª th√¥ng tin c√° nh√¢n ƒë∆∞·ª£c l∆∞u gi·ªØ t·∫°i C∆° s·ªü d·ªØ li·ªáu qu·ªëc gia v·ªÅ d√¢n c∆∞ cho C∆° quan ƒëƒÉng k√Ω kinh doanh, c∆° quan qu·∫£n l√Ω nh√† n∆∞·ªõc v·ªÅ ƒëƒÉng k√Ω kinh doanh ƒë·ªÉ ph·ª•c v·ª• c√¥ng t√°c qu·∫£n l√Ω nh√† n∆∞·ªõc v·ªÅ ƒëƒÉng k√Ω doanh nghi·ªáp theo quy ƒë·ªãnh. C√°c th√¥ng tin k√™ khai ƒë∆∞·ª£c ƒë·ªëi chi·∫øu v·ªõi c√°c th√¥ng tin ƒë∆∞·ª£c l∆∞u gi·ªØ t·∫°i C∆° s·ªü d·ªØ li·ªáu qu·ªëc gia v·ªÅ d√¢n c∆∞. Tr∆∞·ªùng h·ª£p th√¥ng tin c√° nh√¢n ƒë∆∞·ª£c k√™ khai kh√¥ng th·ªëng nh·∫•t so v·ªõi th√¥ng tin ƒë∆∞·ª£c l∆∞u gi·ªØ t·∫°i C∆° s·ªü d·ªØ li·ªáu qu·ªëc gia v·ªÅ d√¢n c∆∞, c√° nh√¢n ƒë√≥ c√≥ tr√°ch nhi·ªám c·∫≠p nh·∫≠t, ƒëi·ªÅu ch·ªânh th√¥ng tin ƒë·ªÉ ƒë·∫£m b·∫£o ch√≠nh x√°c, th·ªëng nh·∫•t.')\n",
    "for sentence in doc.sentences:\n",
    "    print(sentence.constituency)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31cef63",
   "metadata": {},
   "source": [
    "(ROOT (S (S (NP (N Tr∆∞·ªùng h·ª£p)) (VP (Adv ƒë√£) (V c√≥) (NP (N s·ªë) (VP (V ƒë·ªãnh danh) (NP (N c√° nh√¢n)))))) (, ,) (S (NP (N ng∆∞·ªùi) (VP (V th·ª±c hi·ªán) (NP (N th·ªß t·ª•c) (VP (V ƒëƒÉng k√Ω) (NP (N doanh nghi·ªáp)))))) (VP (VP (V k√™ khai) (NP (Det c√°c) (N th√¥ng tin) (PP (Pre v·ªÅ) (NP (NP (N h·ªç)) (, ,) (NP (N ch·ªØ) (V ƒë·ªám)) (CC v√†) (NP (N t√™n)) (, ,) (NP (N ng√†y)) (, ,) (NP (N th√°ng)) (, ,) (NP (N nƒÉm) (N sinh)) (, ,) (NP (N s·ªë) (VP (V ƒë·ªãnh danh) (NP (N c√° nh√¢n)))) (, ,) (NP (N gi·ªõi t√≠nh)) (PP (Pre c·ªßa) (NP (N m√¨nh))) (CC v√†) (PP (Pre c·ªßa) (NP (N c√° nh√¢n))))))) (VP (V c√≥) (NP (V y√™u c·∫ßu) (VP (V k√™ khai) (NP (N th√¥ng tin) (Adj c√° nh√¢n) (PP (Pre trong) (NP (N h·ªì s∆°) (VP (V ƒëƒÉng k√Ω) (NP (N doanh nghi·ªáp))))))))) (CC v√†) (VP (V ƒë·ªìng √Ω) (VP (V chia s·∫ª) (NP (N th√¥ng tin) (Adj c√° nh√¢n) (VP (AUX ƒë∆∞·ª£c) (SBAR (S (VP (V l∆∞u gi·ªØ) (PP (Pre t·∫°i) (NP (N C∆° s·ªü d·ªØ li·ªáu) (N qu·ªëc gia) (PP (Pre v·ªÅ) (NP (N d√¢n c∆∞))))) (PP (Pre cho) (NP (NP (N C∆° quan) (VP (V ƒëƒÉng k√Ω) (V kinh doanh))) (, ,) (NP (N c∆° quan) (VP (V qu·∫£n l√Ω) (NP (N nh√† n∆∞·ªõc)))) (PP (Pre v·ªÅ) (VP (V ƒëƒÉng k√Ω) (V kinh doanh) (PP (Pre ƒë·ªÉ) (VP (V ph·ª•c v·ª•) (NP (N c√¥ng t√°c) (VP (V qu·∫£n l√Ω) (NP (N nh√† n∆∞·ªõc))) (PP (Pre v·ªÅ) (VP (V ƒëƒÉng k√Ω) (NP (N doanh nghi·ªáp)) (VP (Pre theo) (VP (N quy ƒë·ªãnh))))))))))))))))))))) (. .)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b45c76e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(ROOT (S (NP (N N·ªôi dung) (Adj c·ª• th·ªÉ) (PP (Pre c·ªßa) (NP (N ng√†nh) (N kinh t·∫ø) (NP (V c·∫•p) (Num b·ªën)) (VP (V quy ƒë·ªãnh) (PP (Pre t·∫°i) (NP (N kho·∫£n) (Num 1) (NP (N ƒëi·ªÅu) (Pro n√†y)))))))) (VP (VP (V th·ª±c hi·ªán) (VP (V theo) (NP (N quy·∫øt ƒë·ªãnh) (PP (Pre c·ªßa) (NP (N Th·ªß t∆∞·ªõng) (N Ch√≠nh ph·ªß)))))) (VP (V ban h√†nh) (NP (N H·ªá th·ªëng) (NP (N ng√†nh) (N kinh t·∫ø) (Ny Vi·ªát Nam.)))))))\n"
     ]
    }
   ],
   "source": [
    "doc = nlp('N·ªôi dung c·ª• th·ªÉ c·ªßa ng√†nh kinh t·∫ø c·∫•p b·ªën quy ƒë·ªãnh t·∫°i kho·∫£n 1 ƒëi·ªÅu n√†y th·ª±c hi·ªán theo quy·∫øt ƒë·ªãnh c·ªßa Th·ªß t∆∞·ªõng Ch√≠nh ph·ªß ban h√†nh H·ªá th·ªëng ng√†nh kinh t·∫ø Vi·ªát Nam.')\n",
    "for sentence in doc.sentences:\n",
    "    print(sentence.constituency)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "295f1f56",
   "metadata": {},
   "source": [
    "(ROOT (S (NP (N N·ªôi dung) (Adj c·ª• th·ªÉ) (PP (Pre c·ªßa) (NP (N ng√†nh) (N kinh t·∫ø) (NP (V c·∫•p) (Num b·ªën)) (VP (V quy ƒë·ªãnh) (PP (Pre t·∫°i) (NP (N kho·∫£n) (Num 1) (NP (N ƒëi·ªÅu) (Pro n√†y)))))))) (VP (VP (V th·ª±c hi·ªán) (VP (V theo) (NP (N quy·∫øt ƒë·ªãnh) (PP (Pre c·ªßa) (NP (N Th·ªß t∆∞·ªõng) (N Ch√≠nh ph·ªß)))))) (VP (V ban h√†nh) (NP (N H·ªá th·ªëng) (NP (N ng√†nh) (N kinh t·∫ø) (Ny Vi·ªát Nam.)))))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cf8b033b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(ROOT (S (NP (N C∆° s·ªü) (VP (V kh√°m ch·ªØa) (NP (N b·ªánh)))) (VP (AUX ph·∫£i) (VP (V thu ph√≠) (VP (V kh√°m b·ªánh) (PP (Pre ƒë·ªëi v·ªõi) (NP (N ng∆∞·ªùi) (VP (Adv ch∆∞a) (V c√≥) (NP (N th·∫ª) (V b·∫£o hi·ªÉm y t·∫ø)))))))) (. .)))\n"
     ]
    }
   ],
   "source": [
    "doc = nlp('C∆° s·ªü kh√°m ch·ªØa b·ªánh ph·∫£i thu ph√≠ kh√°m b·ªánh ƒë·ªëi v·ªõi ng∆∞·ªùi ch∆∞a c√≥ th·∫ª b·∫£o hi·ªÉm y t·∫ø.')\n",
    "for sentence in doc.sentences:\n",
    "    print(sentence.constituency)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdfdf8df",
   "metadata": {},
   "source": [
    "(ROOT (S (NP (N C∆° s·ªü) (VP (V kh√°m ch·ªØa) (NP (N b·ªánh)))) (VP (AUX ph·∫£i) (VP (V thu ph√≠) (VP (V kh√°m b·ªánh) (PP (Pre ƒë·ªëi v·ªõi) (NP (N ng∆∞·ªùi) (VP (Adv ch∆∞a) (V c√≥) (NP (N th·∫ª) (V b·∫£o hi·ªÉm y t·∫ø)))))))) (. .)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c161a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import stanza\n",
    "from stanza.models.constituency.parse_tree import Tree\n",
    "\n",
    "nlp = stanza.Pipeline(lang='vi', processors='tokenize,pos,constituency')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "c381ddb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(ROOT (S (NP (N ƒë√° qu√Ω) (, ,) (N v√†ng)) (VP (V l√†) (NP (N ƒë·ªì) (N trang s·ª©c) (Adj hi·∫øm)))))\n"
     ]
    }
   ],
   "source": [
    "text = \"ƒë√° qu√Ω, v√†ng l√† ƒë·ªì trang s·ª©c hi·∫øm\"\n",
    "\n",
    "doc = nlp(text)\n",
    "\n",
    "for sentence in doc.sentences:\n",
    "    print(sentence.constituency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5c7a239f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "(VP (V ƒêƒÉng k√Ω) (NP (N doanh nghi·ªáp)))\n",
      "=== SUBJECT (NP) ===\n",
      "None\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'is_leaf'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[51], line 33\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28mprint\u001b[39m(predicate_tree)\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=== SUBJECT (NP) ===\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 33\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mflatten_tree\u001b[49m\u001b[43m(\u001b[49m\u001b[43msubject_tree\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m=== PREDICATE (VP) ===\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28mprint\u001b[39m(flatten_tree(predicate_tree))\n",
      "Cell \u001b[0;32mIn[51], line 16\u001b[0m, in \u001b[0;36mflatten_tree\u001b[0;34m(tree)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Chuy·ªÉn c√¢y th√†nh chu·ªói text\"\"\"\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(tree)\n\u001b[0;32m---> 16\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mtree\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_leaf\u001b[49m():\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tree\u001b[38;5;241m.\u001b[39mlabel\n\u001b[1;32m     18\u001b[0m parts \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'is_leaf'"
     ]
    }
   ],
   "source": [
    "def get_np_vp(tree):\n",
    "    subject = None\n",
    "    predicate = None\n",
    "    # duy·ªát c√°c con c·ªßa node S\n",
    "    for child in tree.children:\n",
    "        label = child.label\n",
    "        if label == \"NP\" and subject is None:\n",
    "            subject = child\n",
    "        elif label == \"VP\" and predicate is None:\n",
    "            predicate = child\n",
    "    return subject, predicate\n",
    "\n",
    "def flatten_tree(tree):\n",
    "    \"\"\"Chuy·ªÉn c√¢y th√†nh chu·ªói text\"\"\"\n",
    "    print(tree)\n",
    "    if tree.is_leaf():\n",
    "        return tree.label\n",
    "    parts = []\n",
    "    for child in tree.children:\n",
    "        parts.append(flatten_tree(child))\n",
    "    return \" \".join(parts)\n",
    "\n",
    "# L·∫•y c√¢u ƒë·∫ßu ti√™n\n",
    "sent = doc.sentences[0]\n",
    "tree = sent.constituency\n",
    "\n",
    "# extract subject + predicate\n",
    "subject_tree, predicate_tree = get_np_vp(tree.children[0])  # ROOT -> S\n",
    "print(subject_tree)\n",
    "print(predicate_tree)\n",
    "\n",
    "print(\"=== SUBJECT (NP) ===\")\n",
    "print(flatten_tree(subject_tree))\n",
    "\n",
    "print(\"\\n=== PREDICATE (VP) ===\")\n",
    "print(flatten_tree(predicate_tree))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "fbe38621",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(NP (N C·ªïng) (N th√¥ng tin) (N ƒëi·ªán t·ª≠) (NP (Num m·ªôt) (N c·ª≠a) (PP (V v·ªÅ) (NP (N tr√≠ tu·ªá nh√¢n t·∫°o)))))\n",
      "NP\n",
      "(VP (VP (V l√†) (NP (N h·ªá th·ªëng) (N th√†nh ph·∫ßn) (PP (Pre c·ªßa) (NP (N C·ªïng) (N d·ªãch v·ª•) (NP (N c√¥ng qu·ªëc) (Nb gia)))))) (, ,) (VP (V th·ª±c hi·ªán) (VP (V ti·∫øp nh·∫≠n) (, ,) (V x·ª≠ l√Ω) (NP (N h·ªì s∆°)))) (, ,) (VP (V cung c·∫•p) (NP (N d·ªãch v·ª•) (N c√¥ng) (AP (Adj tr·ª±c tuy·∫øn)))) (CC v√†) (VP (Adj c√¥ng khai) (NP (N th√¥ng tin)) (VP (Pre theo) (NP (N quy ƒë·ªãnh) (PP (Pre c·ªßa) (NP (N ph√°p lu·∫≠t)))))))\n",
      "VP\n",
      "(. .)\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "text = '''\n",
    "C·ªïng th√¥ng tin ƒëi·ªán t·ª≠ m·ªôt c·ª≠a v·ªÅ tr√≠ tu·ªá nh√¢n t·∫°o l√† h·ªá th·ªëng th√†nh ph·∫ßn c·ªßa C·ªïng d·ªãch v·ª• c√¥ng qu·ªëc gia, th·ª±c hi·ªán ti·∫øp nh·∫≠n, x·ª≠ l√Ω h·ªì s∆°, cung c·∫•p d·ªãch v·ª• c√¥ng tr·ª±c tuy·∫øn v√† c√¥ng khai th√¥ng tin theo quy ƒë·ªãnh c·ªßa ph√°p lu·∫≠t.\n",
    "'''\n",
    "\n",
    "doc = nlp(text)\n",
    "\n",
    "tree = doc.sentences[0].constituency\n",
    "\n",
    "import re\n",
    "\n",
    "ignore_sentence = [\"Aux\", \"Num\", \"\"]\n",
    "\n",
    "tmp = tree.children[0]\n",
    "for child in tmp.children:\n",
    "    print(child)\n",
    "    print(child.label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd52878",
   "metadata": {},
   "source": [
    "(NP (NP (N Tr∆∞·ªùng h·ª£p) (SBAR (S (NP (N c√¥ng ty) (VP (N tr√°ch nhi·ªám) (NP (V h·ªØu h·∫°n) (CC v√†) (N c√¥ng ty c·ªï ph·∫ßn)))) (VP (V c√≥) (AP (Adj nhi·ªÅu) (Adv h∆°n)))))) (CC v√†) (NP (Num m·ªôt) (N ng∆∞·ªùi) (VP (V ƒë·∫°i di·ªán) (VP (Pre theo) (NP (N ph√°p lu·∫≠t))))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e40481a4",
   "metadata": {},
   "source": [
    "ƒêƒÉng k√Ω doanh nghi·ªáp l√† vi·ªác ƒëƒÉng k√Ω n·ªôi dung ƒëƒÉng k√Ω kinh doanh v√† ƒëƒÉng k√Ω thu·∫ø c·ªßa doanh nghi·ªáp d·ª± ki·∫øn th√†nh l·∫≠p, ƒëƒÉng k√Ω thay ƒë·ªïi n·ªôi dung ƒëƒÉng k√Ω doanh nghi·ªáp, ƒëƒÉng k√Ω ho·∫°t ƒë·ªông v√† ƒëƒÉng k√Ω thay ƒë·ªïi n·ªôi dung ƒëƒÉng k√Ω ho·∫°t ƒë·ªông chi nh√°nh, vƒÉn ph√≤ng ƒë·∫°i di·ªán, ƒë·ªãa ƒëi·ªÉm kinh doanh c·ªßa doanh nghi·ªáp, c√°c nghƒ©a v·ª• ƒëƒÉng k√Ω, th√¥ng b√°o kh√°c v√† c√°c nghi·ªáp v·ª• c√≥ li√™n quan theo quy ƒë·ªãnh c·ªßa Lu·∫≠t Doanh nghi·ªáp v√† Ngh·ªã ƒë·ªãnh n√†y"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
