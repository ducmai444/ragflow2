{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "188ef8ec",
   "metadata": {},
   "source": [
    "#### Test Classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1041cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "classifier = pipeline(\"zero-shot-classification\",\n",
    "                      model=\"joeddav/xlm-roberta-large-xnli\",\n",
    "                      use_fast=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0308e68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "clause_1 = \"\"\"c) ƒê·∫ßu t∆∞ ph√°t tri·ªÉn VLXD tr√™n ƒë·ªãa b√†n t·ªânh theo nhu c·∫ßu c·ªßa th·ªã tr∆∞·ªùng v√† c√°c quy ho·∫°ch, ƒë·ªÅ √°n, k·∫ø ho·∫°ch ƒë∆∞·ª£c duy·ªát; kh√¥ng ƒë·∫ßu t∆∞ c√°c d·ª± √°n s·∫£n xu·∫•t VLXD ·ªü c√°c v√πng ·∫£nh h∆∞·ªüng ƒë·∫øn h√†nh lang b·∫£o v·ªá c√¥ng tr√¨nh qu·ªëc ph√≤ng, an ninh, giao th√¥ng, thu·ª∑ l·ª£i, ƒë√™ ƒëi·ªÅu, nƒÉng l∆∞·ª£ng, khu di t√≠ch, l·ªãch s·ª≠ - vƒÉn h√≥a v√† khu v·ª±c b·∫£o v·ªá c√¥ng tr√¨nh kh√°c theo quy ƒë·ªãnh c·ªßa ph√°p lu·∫≠t.\"\"\"\n",
    "\n",
    "clause_2 = \"\"\"g) C√°c d·ª± √°n s·∫£n xu·∫•t v·∫≠t li·ªáu x√¢y d·ª±ng tr√™n ƒë·ªãa b√†n t·ªânh v·∫´n ƒë∆∞·ª£c ph√©p ƒë·∫ßu t∆∞ t·∫°i c√°c khu v·ª±c n·∫±m trong h√†nh lang b·∫£o v·ªá c√¥ng tr√¨nh qu·ªëc ph√≤ng, an ninh, giao th√¥ng, thu·ª∑ l·ª£i, ƒë√™ ƒëi·ªÅu, nƒÉng l∆∞·ª£ng, di t√≠ch l·ªãch s·ª≠ ‚Äì vƒÉn h√≥a v√† c√°c khu v·ª±c b·∫£o v·ªá c√¥ng tr√¨nh kh√°c n·∫øu c√≥ nhu c·∫ßu ph√°t tri·ªÉn th·ªã tr∆∞·ªùng v√† ph√π h·ª£p v·ªõi quy ho·∫°ch.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a4183b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "model_name = \"MoritzLaurer/mDeBERTa-v3-base-xnli-multilingual-nli-2mil7\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "model.to(device)\n",
    "\n",
    "premise = clause_1\n",
    "hypothesis = clause_2\n",
    "\n",
    "inputs = tokenizer(premise, hypothesis, truncation=True, return_tensors=\"pt\")\n",
    "inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "output = model(**inputs)\n",
    "prediction = torch.softmax(output[\"logits\"][0], -1).tolist()\n",
    "label_names = [\"entailment\", \"neutral\", \"contradiction\"]\n",
    "prediction = {name: round(float(pred) * 100, 1) for pred, name in zip(prediction, label_names)}\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0caa492c",
   "metadata": {},
   "outputs": [],
   "source": [
    "premise = clause_1\n",
    "hypothesis = clause_2\n",
    "\n",
    "inputs = tokenizer(premise, hypothesis, truncation=True, return_tensors=\"pt\")\n",
    "inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "output = model(**inputs)\n",
    "prediction = torch.softmax(output[\"logits\"][0], -1).tolist()\n",
    "label_names = [\"entailment\", \"neutral\", \"contradiction\"]\n",
    "prediction = {name: round(float(pred) * 100, 1) for pred, name in zip(prediction, label_names)}\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba6da454",
   "metadata": {},
   "source": [
    "#### Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba000673",
   "metadata": {},
   "outputs": [],
   "source": [
    "clause_1 = \"\"\"c) ƒê·∫ßu t∆∞ ph√°t tri·ªÉn VLXD tr√™n ƒë·ªãa b√†n t·ªânh theo nhu c·∫ßu c·ªßa th·ªã tr∆∞·ªùng v√† c√°c quy ho·∫°ch, ƒë·ªÅ √°n, k·∫ø ho·∫°ch ƒë∆∞·ª£c duy·ªát; kh√¥ng ƒë·∫ßu t∆∞ c√°c d·ª± √°n s·∫£n xu·∫•t VLXD ·ªü c√°c v√πng ·∫£nh h∆∞·ªüng ƒë·∫øn h√†nh lang b·∫£o v·ªá c√¥ng tr√¨nh qu·ªëc ph√≤ng, an ninh, giao th√¥ng, thu·ª∑ l·ª£i, ƒë√™ ƒëi·ªÅu, nƒÉng l∆∞·ª£ng, khu di t√≠ch, l·ªãch s·ª≠ - vƒÉn h√≥a v√† khu v·ª±c b·∫£o v·ªá c√¥ng tr√¨nh kh√°c theo quy ƒë·ªãnh c·ªßa ph√°p lu·∫≠t.\"\"\"\n",
    "\n",
    "clause_2 = \"\"\"g) C√°c d·ª± √°n s·∫£n xu·∫•t v·∫≠t li·ªáu x√¢y d·ª±ng tr√™n ƒë·ªãa b√†n t·ªânh v·∫´n ƒë∆∞·ª£c ph√©p ƒë·∫ßu t∆∞ t·∫°i c√°c khu v·ª±c n·∫±m trong h√†nh lang b·∫£o v·ªá c√¥ng tr√¨nh qu·ªëc ph√≤ng, an ninh, giao th√¥ng, thu·ª∑ l·ª£i, ƒë√™ ƒëi·ªÅu, nƒÉng l∆∞·ª£ng, di t√≠ch l·ªãch s·ª≠ ‚Äì vƒÉn h√≥a v√† c√°c khu v·ª±c b·∫£o v·ªá c√¥ng tr√¨nh kh√°c n·∫øu c√≥ nhu c·∫ßu ph√°t tri·ªÉn th·ªã tr∆∞·ªùng v√† ph√π h·ª£p v·ªõi quy ho·∫°ch.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1140a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "from pyvi import ViTokenizer\n",
    "\n",
    "# Download from the ü§ó Hub\n",
    "model = SentenceTransformer(\"huyydangg/DEk21_hcmute_embedding\")\n",
    "\n",
    "# Define query (c√¢u h·ªèi ph√°p lu·∫≠t) v√† docs (ƒëi·ªÅu lu·∫≠t)\n",
    "query = clause_1\n",
    "docs = [clause_2]\n",
    "\n",
    "# T√°ch t·ª´ cho query\n",
    "segmented_query = ViTokenizer.tokenize(query)\n",
    "\n",
    "# T√°ch t·ª´ cho t·ª´ng d√≤ng vƒÉn b·∫£n\n",
    "segmented_docs = [ViTokenizer.tokenize(doc) for doc in docs]\n",
    "\n",
    "# Encode query and documents\n",
    "query_embedding = model.encode([segmented_query])\n",
    "doc_embeddings = model.encode(segmented_docs)\n",
    "similarities = torch.nn.functional.cosine_similarity(\n",
    "    torch.tensor(query_embedding), torch.tensor(doc_embeddings)\n",
    ").flatten()\n",
    "\n",
    "# Sort documents by cosine similarity\n",
    "sorted_indices = torch.argsort(similarities, descending=True)\n",
    "sorted_docs = [docs[idx] for idx in sorted_indices]\n",
    "sorted_scores = [similarities[idx].item() for idx in sorted_indices]\n",
    "\n",
    "# Print sorted documents with their cosine scores\n",
    "for doc, score in zip(sorted_docs, sorted_scores):\n",
    "    print(f\"Document: {doc} - Cosine Similarity: {score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ccf4e72",
   "metadata": {},
   "source": [
    "#### Get document\n",
    "\n",
    "1. Extract clause area\n",
    "2. Using cosine similarity\n",
    "3. Using \"MoritzLaurer/mDeBERTa-v3-base-xnli-multilingual-nli-2mil7\" model in three labels: \"entailment\", \"neutral\", \"contradiction\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc37b7b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import docx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from level_detector import LevelDetector\n",
    "\n",
    "level_detector = LevelDetector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723038c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_path = \"/home/ducmb/fis_legal_nlp/F1/test.docx\"\n",
    "\n",
    "paragraphs = docx.Document(temp_path).paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c24155a",
   "metadata": {},
   "outputs": [],
   "source": [
    "begin = 0\n",
    "n = len(paragraphs)\n",
    "clauses = []\n",
    "sample_stop = ['m·∫´u b√°o c√°o', 'm·∫´u bi√™n b·∫£n', 'm·∫´u c√¥ng vƒÉn', 'm·∫´u gi·∫•y gi·ªõi thi·ªáu', 'm·∫´u k·∫ø ho·∫°ch', 'm·∫´u quy·∫øt ƒë·ªãnh c√° bi·ªát ban h√†nh vƒÉn b·∫£n k√®m theo']\n",
    "\n",
    "while begin < n:\n",
    "    heading = paragraphs[begin].text\n",
    "    level = level_detector.get_level(heading)\n",
    "\n",
    "    if level['level_id'] == 0 and any(stop in heading.lower() for stop in sample_stop):\n",
    "        break\n",
    "\n",
    "    if level['type'] != 'ƒëi·ªÅu':\n",
    "        begin += 1\n",
    "        continue\n",
    "\n",
    "    current_entry = [heading]\n",
    "    clauses.append(current_entry)   \n",
    "\n",
    "    next_idx = begin + 1\n",
    "    while next_idx < n:\n",
    "        body_text = paragraphs[next_idx].text\n",
    "        if body_text == '':\n",
    "            next_idx += 1\n",
    "            continue\n",
    "        body_level = level_detector.get_level(body_text)\n",
    "\n",
    "        if body_level['level_id'] < 3 or body_level['type'] == 'ƒëi·ªÅu' or body_text in [' ', '\\xa0']:\n",
    "            break\n",
    "\n",
    "        current_entry.append(body_text)\n",
    "        next_idx += 1\n",
    "\n",
    "\n",
    "    begin = next_idx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34e6fd8",
   "metadata": {},
   "source": [
    "#### Test chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a62d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from FisReader.document_factory import DocumentFactory\n",
    "from ChunkExtractor.chunk_extractor import ChunkExtractor\n",
    "import pathlib\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = pathlib.Path(\"/home/maibaduc/nlp/LegalChunk/data/02_2011_QD_KTNN_m_127602.docx\")\n",
    "doc_id = \"1\"\n",
    "\n",
    "document = DocumentFactory(ocr_endpoint=\"http://10.15.84.44:6999\").read(file_path, doc_id, for_llm=True)\n",
    "with open(\"tree_test.json\", \"w\", encoding='utf8') as f:\n",
    "    json.dump(document.tree_structure, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "\n",
    "chunk_extractor = ChunkExtractor()\n",
    "\n",
    "chunks = chunk_extractor.get_chunks_in_tree(document, doc_id, file_path, \"data.docx\")\n",
    "\n",
    "print(type(chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e2896c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk in chunks:\n",
    "    print(chunk)\n",
    "    print(\"*\"*100)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47c60b2",
   "metadata": {},
   "source": [
    "#### End of chunking test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5862c776",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "from pyvi import ViTokenizer\n",
    "\n",
    "embed_model = SentenceTransformer(\"huyydangg/DEk21_hcmute_embedding\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec39674",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_article_structure(article_groups):\n",
    "    \"\"\"\n",
    "    Extract hierarchical legal references (Article ‚Üí Clause ‚Üí Point)\n",
    "    from pre-parsed document segments.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    article_groups : list[list[str]]\n",
    "        A list where each element represents one article block.\n",
    "        Each article block contains a list of raw segments (ƒêi·ªÅu, Kho·∫£n, ƒêi·ªÉm, ...).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list[list[str]]\n",
    "        A nested list where each article group is mapped to a list of\n",
    "        fully-qualified legal references, such as:\n",
    "            - \"ƒêi·ªÅu 12\"\n",
    "            - \"Kho·∫£n 2 ƒêi·ªÅu 12\"\n",
    "            - \"ƒêi·ªÉm b Kho·∫£n 2 ƒêi·ªÅu 12\"\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    - If a clause contains points, only the points will be added (not the clause itself).\n",
    "    - If a clause has no points, the clause will be added to the output.\n",
    "    - The function respects the legal hierarchy: Article > Clause > Point.\n",
    "    \"\"\"\n",
    "\n",
    "    final_results = []\n",
    "\n",
    "    for group in article_groups:\n",
    "        output = []\n",
    "\n",
    "        if len(group) == 1:\n",
    "            output.append(group[0])\n",
    "        elif len(group) == 2:\n",
    "            article_text = group[0]\n",
    "            clause_text = group[1]\n",
    "            output.append(f\"{clause_text} {article_text}\")\n",
    "        # Case: complex structure: ƒêi·ªÅu ‚Äì Kho·∫£n ‚Äì ƒêi·ªÉm\n",
    "        else:\n",
    "            article = \"\"\n",
    "            clause = \"\"\n",
    "            point = \"\"\n",
    "            sub_point = \"\"\n",
    "            output = []\n",
    "\n",
    "            for index, doc in enumerate(group):\n",
    "                curr_level = level_detector.get_level(doc)\n",
    "                curr_level_type = curr_level[\"type\"]\n",
    "                curr_level_id = curr_level[\"level_id\"]\n",
    "\n",
    "                # L·∫•y level c·ªßa ph·∫ßn t·ª≠ ti·∫øp theo (ho·∫∑c 0 n·∫øu l√† ph·∫ßn t·ª≠ cu·ªëi)\n",
    "                if index + 1 < len(group):\n",
    "                    next_level = level_detector.get_level(group[index + 1])\n",
    "                    next_level_id = next_level[\"level_id\"]\n",
    "                else:\n",
    "                    next_level_id = 0  # sentinel: coi nh∆∞ h·∫øt vƒÉn b·∫£n ‚Üí bu·ªôc flush\n",
    "\n",
    "                # ------------------ ƒêI·ªÄU ------------------\n",
    "                if curr_level_type == \"ƒëi·ªÅu\":\n",
    "                    # Khi g·∫∑p ƒêi·ªÅu m·ªõi ‚Üí reset context th·∫•p h∆°n\n",
    "                    article = doc\n",
    "                    clause = \"\"\n",
    "                    point = \"\"\n",
    "                    sub_point = \"\"\n",
    "\n",
    "                    # N·∫øu ƒêi·ªÅu n√†y kh√¥ng c√≥ Kho·∫£n/ƒêi·ªÉm ph√≠a sau\n",
    "                    # (v√≠ d·ª•: ƒêi·ªÅu X., sau ƒë√≥ l√† ƒêi·ªÅu Y. ho·∫∑c h·∫øt lu√¥n)\n",
    "                    if curr_level_id >= next_level_id:\n",
    "                        output.append(article)\n",
    "\n",
    "                # ------------------ KHO·∫¢N ------------------\n",
    "                elif curr_level_type == \"kho·∫£n\":\n",
    "                    clause = \"Kho·∫£n \" + doc\n",
    "                    point = \"\"\n",
    "                    sub_point = \"\"\n",
    "\n",
    "                    # N·∫øu sau Kho·∫£n kh√¥ng c√≤n c·∫•p th·∫•p h∆°n (ƒëi·ªÉm/ƒëi·ªÉm con/n·ªôi dung)\n",
    "                    # m√† nh·∫£y sang Kho·∫£n kh√°c, ƒêi·ªÅu kh√°c ho·∫∑c h·∫øt vƒÉn b·∫£n\n",
    "                    if curr_level_id >= next_level_id:\n",
    "                        parts = [clause, article]  # \"1. Kho·∫£n ƒêi·ªÅu 1.\"\n",
    "                        output.append(\" \".join(parts))\n",
    "\n",
    "                # ------------------ ƒêI·ªÇM ------------------\n",
    "                elif curr_level_type == \"ƒëi·ªÉm\":\n",
    "                    point = \"ƒêi·ªÉm \" + doc\n",
    "                    sub_point = \"\"\n",
    "\n",
    "                    if curr_level_id >= next_level_id:\n",
    "                        parts = [point]\n",
    "                        if clause:\n",
    "                            parts.append(clause)\n",
    "                        if article:\n",
    "                            parts.append(article)\n",
    "                        output.append(\" \".join(parts))\n",
    "\n",
    "                # ------------------ ƒêI·ªÇM CON ------------------\n",
    "                elif curr_level_type == \"ƒëi·ªÉm con\":\n",
    "                    sub_point = doc\n",
    "\n",
    "                    if curr_level_id >= next_level_id:\n",
    "                        parts = [sub_point]\n",
    "                        if point:\n",
    "                            parts.append(point)\n",
    "                        if clause:\n",
    "                            parts.append(clause)\n",
    "                        if article:\n",
    "                            parts.append(article)\n",
    "                        output.append(\" \".join(parts))\n",
    "\n",
    "                # ------------------ N·ªòI DUNG ------------------\n",
    "                elif curr_level_type == \"n·ªôi dung\":\n",
    "                    # n·ªôi dung chi ti·∫øt n·∫±m d∆∞·ªõi c√πng\n",
    "                    if curr_level_id >= next_level_id:\n",
    "                        parts = [doc]\n",
    "                        if sub_point:\n",
    "                            parts.append(sub_point)\n",
    "                        if point:\n",
    "                            parts.append(point)\n",
    "                        if clause:\n",
    "                            parts.append(clause)\n",
    "                        if article:\n",
    "                            parts.append(article)\n",
    "                        output.append(\" \".join(parts))\n",
    "\n",
    "        final_results.append(output)\n",
    "\n",
    "    return final_results\n",
    "\n",
    "# Example usage:\n",
    "article_hierarchy = extract_article_structure(clauses)\n",
    "article_hierarchy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a44024b",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_article_hierarchy = []\n",
    "\n",
    "for docs in article_hierarchy:\n",
    "    segmented_docs = [ViTokenizer.tokenize(doc) for doc in docs]\n",
    "    embed_docs = embed_model.encode(segmented_docs)\n",
    "    embed_article_hierarchy.append(embed_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2cc98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "pairs = list(combinations(range(len(article_hierarchy)), 2))\n",
    "print(pairs)\n",
    "print(len(pairs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d17ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_pairs(article_hierarchy, embed_article_hierarchy, pairs):\n",
    "    result = {}\n",
    "    for pair in pairs:\n",
    "        ind_0, ind_1 = pair[0], pair[1]\n",
    "        embed_1 = embed_article_hierarchy[ind_0]\n",
    "        embed_2 = embed_article_hierarchy[ind_1]\n",
    "        for i in range(len(embed_1)):\n",
    "            similarities = torch.nn.functional.cosine_similarity(\n",
    "                torch.tensor(embed_1[i]), torch.tensor(embed_2)\n",
    "            ).flatten()\n",
    "            temp_index = [index for index, value in enumerate(similarities) if value > 0.4]\n",
    "            \n",
    "            if len(temp_index) > 0:\n",
    "                key = article_hierarchy[ind_0][i]\n",
    "                values = [article_hierarchy[ind_1][index] for index in temp_index]\n",
    "\n",
    "                if key not in result:\n",
    "                    result[key] = values\n",
    "                else:\n",
    "                    result[key].extend(values)\n",
    "    return result\n",
    "\n",
    "best_article_pairs = get_best_pairs(article_hierarchy, embed_article_hierarchy, pairs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0321f81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = \"d) Lu·∫≠t n√†y kh√¥ng ƒëi·ªÅu ch·ªânh ho·∫°t ƒë·ªông nghi√™n c·ª©u v√† ph√°t tri·ªÉn h·ªá th·ªëng tr√≠ tu·ªá nh√¢n t·∫°o.\"\n",
    "\n",
    "keys = list(best_article_pairs.keys())\n",
    "\n",
    "for key, values in best_article_pairs.items():\n",
    "    if key == temp:\n",
    "        print(values)\n",
    "    for value in values:\n",
    "        if value == temp:\n",
    "            print(key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64fe06d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "keys_pairs = list(best_article_pairs.keys())\n",
    "print(keys_pairs[0])\n",
    "print(len(list(best_article_pairs.values())))\n",
    "best_article_pairs[keys_pairs[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6bd652d",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_1 = embed_article_hierarchy[0]\n",
    "embed_2 = embed_article_hierarchy[3]\n",
    "\n",
    "print(embed_1.shape)\n",
    "print(embed_2.shape)\n",
    "\n",
    "similarities = torch.nn.functional.cosine_similarity(\n",
    "    torch.tensor(embed_1[0]), torch.tensor(embed_2)\n",
    ").flatten()\n",
    "\n",
    "temp_index = [index for index, value in enumerate(similarities) if value > 0.5]\n",
    "print(temp_index)\n",
    "\n",
    "if len(temp_index) == 0:\n",
    "    print(\"No similar pairs found\")\n",
    "\n",
    "sorted_indices = torch.argsort(similarities, descending=True)\n",
    "\n",
    "print(sorted_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9377ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "clause_1 = \"\"\n",
    "clause_2 = \"\"\n",
    "\n",
    "for i, (k, v) in enumerate(result.items()):\n",
    "    if i == 1:\n",
    "        clause_1 = k\n",
    "        clause_2 = v[3]\n",
    "    if i > 1:\n",
    "        break\n",
    "\n",
    "print(clause_1)\n",
    "print(clause_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3089962e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "model_name = \"MoritzLaurer/mDeBERTa-v3-base-xnli-multilingual-nli-2mil7\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b0723c",
   "metadata": {},
   "outputs": [],
   "source": [
    "premise = \"\"\"ƒêi·ªÅu 1. Ban h√†nh k√®m theo Quy·∫øt ƒë·ªãnh n√†y Quy ƒë·ªãnh ni√™m phong t√†i li·ªáu, ki·ªÉm tra t√†i kho·∫£n c·ªßa ƒë∆°n v·ªã ƒë∆∞·ª£c ki·ªÉm to√°n v√† c√° nh√¢n c√≥ li√™n quan trong ho·∫°t ƒë·ªông ki·ªÉm to√°n c·ªßa Ki·ªÉm to√°n Nh√† n∆∞·ªõc.\"\"\"\n",
    "hypothesis = \"\"\"Kho·∫£n 1. Th·ª±c hi·ªán c√°c quy·ªÅn v√† tr√°ch nhi·ªám quy ƒë·ªãnh t·∫°i ƒêi·ªÅu 22 c·ªßa Quy ƒë·ªãnh n√†y. ƒêi·ªÅu 23. Quy·ªÅn v√† tr√°ch nhi·ªám c·ªßa ƒë∆°n v·ªã ƒë∆∞·ª£c ki·ªÉm to√°n v√† c√° nh√¢n c√≥ t√†i kho·∫£n b·ªã ki·ªÉm tra\"\"\"\n",
    "\n",
    "inputs = tokenizer(premise, hypothesis, truncation=True, return_tensors=\"pt\")\n",
    "inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "output = model(**inputs)\n",
    "prediction = torch.softmax(output[\"logits\"][0], -1).tolist()\n",
    "label_names = [\"entailment\", \"neutral\", \"contradiction\"]\n",
    "prediction = {name: round(float(pred) * 100, 1) for pred, name in zip(prediction, label_names)}\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f03ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_contract_pair(close_pairs):\n",
    "    result = []\n",
    "\n",
    "    for key, values in close_pairs.items():\n",
    "        for value in values:\n",
    "            premise = key\n",
    "            hypothesis = value\n",
    "            inputs = tokenizer(premise, hypothesis, truncation=True, return_tensors=\"pt\")\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            output = model(**inputs)\n",
    "            prediction = torch.softmax(output[\"logits\"][0], -1).tolist()\n",
    "            label_names = [\"entailment\", \"neutral\", \"contradiction\"]\n",
    "            prediction = {name: round(float(pred) * 100, 1) for pred, name in zip(prediction, label_names)}\n",
    "            if prediction['contradiction'] > prediction['entailment'] and prediction['contradiction'] > prediction['neutral']:\n",
    "                result.append((premise, hypothesis))\n",
    "    return result\n",
    "\n",
    "\n",
    "result_1 = get_contract_pair(result)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601748d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from pydantic import BaseModel\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=\"sk-F79270ATXBohPIr56I4jLA\",\n",
    "    base_url=\"https://mkp-api.fptcloud.com/v1\",\n",
    ")\n",
    "\n",
    "class ConflictResult(BaseModel):\n",
    "    score: float              # numeric score 0 ‚Üí 1\n",
    "    interpretation: str       # detailed explanation\n",
    "\n",
    "\n",
    "prompt_system = (\n",
    "    \"You are a senior legal NLP analyst specialized in Vietnamese regulatory documents. \"\n",
    "    \"Your task is to evaluate the relationship between two legal clauses \"\n",
    "    \"and quantify the degree of internal conflict or contradiction between them. \"\n",
    "    \"Return:\\n\"\n",
    "    \"1) a numerical conflict score between 0.0 and 1.0, where 0.0 means no conflict and 1.0 means full contradiction,\\n\"\n",
    "    \"2) a concise interpretation in Vietnamese explaining the rationale of the score.\\n\"\n",
    "    \"Do not provide legal advice. Stay objective, analytical, and deterministic.\"\n",
    ")\n",
    "\n",
    "def analyze_conflict(clause_a: str, clause_b: str):\n",
    "\n",
    "    response = client.responses.parse(\n",
    "        model=\"gpt-oss-20b\",\n",
    "        input=[\n",
    "            {\"role\": \"system\", \"content\": prompt_system},\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": (\n",
    "                    \"Clause A:\\n\"\n",
    "                    f\"{clause_a}\\n\\n\"\n",
    "                    \"Clause B:\\n\"\n",
    "                    f\"{clause_b}\\n\\n\"\n",
    "                    \"Analyze and score their level of internal conflict.\"\n",
    "                ),\n",
    "            },\n",
    "        ],\n",
    "        text_format=ConflictResult,\n",
    "    )\n",
    "\n",
    "    return response.output_parsed\n",
    "\n",
    "\n",
    "# -------------------\n",
    "# Example Usage\n",
    "# -------------------\n",
    "\n",
    "clause_a = contract_pairs[0][0]\n",
    "clause_b = contract_pairs[0][1]\n",
    "\n",
    "result = analyze_conflict(clause_a, clause_b)\n",
    "\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f586e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(best_article_pairs.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4c27ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "conflict_pairs = []\n",
    "for key, values in best_article_pairs.items():\n",
    "    for value in values:\n",
    "        result = analyze_conflict(key, value)\n",
    "        score =  result.__dict__['score']\n",
    "        if score > 0.5:\n",
    "            conflict_pairs.append((key, value, result.__dict__['interpretation']))\n",
    "\n",
    "conflict_pairs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404408ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel\n",
    "\n",
    "model = AutoModel.from_pretrained(\n",
    "    'jinaai/jina-reranker-v3',\n",
    "    dtype=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189f6f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_1 = \"c) ƒê·∫ßu t∆∞ ph√°t tri·ªÉn VLXD tr√™n ƒë·ªãa b√†n t·ªânh theo nhu c·∫ßu c·ªßa th·ªã tr∆∞·ªùng v√† c√°c quy ho·∫°ch, ƒë·ªÅ √°n, k·∫ø ho·∫°ch ƒë∆∞·ª£c duy·ªát; kh√¥ng ƒë·∫ßu t∆∞ c√°c d·ª± √°n s·∫£n xu·∫•t VLXD ·ªü c√°c v√πng ·∫£nh h∆∞·ªüng ƒë·∫øn h√†nh lang b·∫£o v·ªá c√¥ng tr√¨nh qu·ªëc ph√≤ng, an ninh, giao th√¥ng, thu·ª∑ l·ª£i, ƒë√™ ƒëi·ªÅu, nƒÉng l∆∞·ª£ng, khu di t√≠ch, l·ªãch s·ª≠ - vƒÉn h√≥a v√† khu v·ª±c b·∫£o v·ªá c√¥ng tr√¨nh kh√°c theo quy ƒë·ªãnh c·ªßa ph√°p lu·∫≠t.\"\n",
    "\n",
    "test_2 = \"m) Trong tr∆∞·ªùng h·ª£p c√≥ y√™u c·∫ßu th√∫c ƒë·∫©y tƒÉng tr∆∞·ªüng c√¥ng nghi·ªáp ƒë·ªãa ph∆∞∆°ng, c√°c d·ª± √°n s·∫£n xu·∫•t v·∫≠t li·ªáu x√¢y d·ª±ng c√≥ th·ªÉ xem x√©t tri·ªÉn khai t·∫°i nh·ªØng khu v·ª±c thu·ªôc h√†nh lang b·∫£o v·ªá c√¥ng tr√¨nh qu·ªëc ph√≤ng, an ninh, giao th√¥ng, th·ªßy l·ª£i, ƒë√™ ƒëi·ªÅu, nƒÉng l∆∞·ª£ng, di t√≠ch l·ªãch s·ª≠ ‚Äì vƒÉn h√≥a v√† m·ªôt s·ªë khu v·ª±c b·∫£o v·ªá c√¥ng tr√¨nh kh√°c theo ƒë√°nh gi√° c·ªßa c∆° quan c√≥ th·∫©m quy·ªÅn.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "168a6e51",
   "metadata": {},
   "source": [
    "c) ƒê·∫ßu t∆∞ ph√°t tri·ªÉn VLXD tr√™n ƒë·ªãa b√†n t·ªânh theo nhu c·∫ßu c·ªßa th·ªã tr∆∞·ªùng v√† c√°c quy ho·∫°ch, ƒë·ªÅ √°n, k·∫ø ho·∫°ch ƒë∆∞·ª£c duy·ªát; kh√¥ng ƒë·∫ßu t∆∞ c√°c d·ª± √°n s·∫£n xu·∫•t VLXD ·ªü c√°c v√πng ·∫£nh h∆∞·ªüng ƒë·∫øn h√†nh lang b·∫£o v·ªá c√¥ng tr√¨nh qu·ªëc ph√≤ng, an ninh, giao th√¥ng, thu·ª∑ l·ª£i, ƒë√™ ƒëi·ªÅu, nƒÉng l∆∞·ª£ng, khu di t√≠ch, l·ªãch s·ª≠ - vƒÉn h√≥a v√† khu v·ª±c b·∫£o v·ªá c√¥ng tr√¨nh kh√°c theo quy ƒë·ªãnh c·ªßa ph√°p lu·∫≠t."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae71eb5d",
   "metadata": {},
   "source": [
    "m) Trong tr∆∞·ªùng h·ª£p c√≥ y√™u c·∫ßu th√∫c ƒë·∫©y tƒÉng tr∆∞·ªüng c√¥ng nghi·ªáp ƒë·ªãa ph∆∞∆°ng, c√°c d·ª± √°n s·∫£n xu·∫•t v·∫≠t li·ªáu x√¢y d·ª±ng c√≥ th·ªÉ xem x√©t tri·ªÉn khai t·∫°i nh·ªØng khu v·ª±c thu·ªôc h√†nh lang b·∫£o v·ªá c√¥ng tr√¨nh qu·ªëc ph√≤ng, an ninh, giao th√¥ng, th·ªßy l·ª£i, ƒë√™ ƒëi·ªÅu, nƒÉng l∆∞·ª£ng, di t√≠ch l·ªãch s·ª≠ ‚Äì vƒÉn h√≥a v√† m·ªôt s·ªë khu v·ª±c b·∫£o v·ªá c√¥ng tr√¨nh kh√°c theo ƒë√°nh gi√° c·ªßa c∆° quan c√≥ th·∫©m quy·ªÅn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e78190e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_index = 0\n",
    "\n",
    "query = keys_pairs[test_index]\n",
    "\n",
    "documents = best_article_pairs[keys_pairs[test_index]]\n",
    "\n",
    "# Rerank documents\n",
    "results = model.rerank(query, documents)\n",
    "\n",
    "# Results are sorted by relevance score (highest first)\n",
    "for result in results:\n",
    "    print(f\"Score: {result['relevance_score']:.4f}\")\n",
    "    print(f\"Document: {result['document'][:100]}...\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11420324",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_contract_pair(close_pairs):\n",
    "    contract_pair = []\n",
    "\n",
    "    for index, (key, values) in enumerate(close_pairs.items()):\n",
    "        query = key\n",
    "        documents = values\n",
    "        print(f'Index: {index}')\n",
    "        results = model.rerank(query, documents)\n",
    "        for result in results:\n",
    "            if result['relevance_score'] >  0.5:\n",
    "                contract_pair.append((query, result['document']))\n",
    "    return contract_pair\n",
    "\n",
    "\n",
    "contract_pairs = get_contract_pair(best_article_pairs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc4762f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "\n",
    "def rerank_one(args):\n",
    "    key, docs = args\n",
    "    results = model.rerank(key, docs)\n",
    "    return [(key, r[\"document\"]) for r in results if r[\"relevance_score\"] > 0.5]\n",
    "\n",
    "def get_contract_pair(close_pairs):\n",
    "    with Pool(processes=8) as p:\n",
    "        results = p.map(rerank_one, close_pairs.items())\n",
    "    contract_pairs = [item for sub in results for item in sub]\n",
    "    return contract_pairs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581322f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = [(1, 2), (3, 4)] \n",
    "\n",
    "for k, v in temp:\n",
    "    print(k,v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b97e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "contract_pairs = get_contract_pair(best_article_pairs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7074f722",
   "metadata": {},
   "source": [
    "#### Test on ContractNLI Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8f3c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8baacd5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "pipe = pipeline(\"text-classification\",model=\"tasksource/deberta-base-long-nli\")\n",
    "out = pipe([dict(text='there is a cat', text_pair='there is a black cat'), \n",
    "dict(text=\"there is a cat\", text_pair=\"there is a black cat\")]) #list of (premise,hypothesis)\n",
    "# [{'label': 'neutral', 'score': 0.9952911138534546}] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c860731c",
   "metadata": {},
   "outputs": [],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc97d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "\n",
    "model_name = \"VietAI/envit5-translation\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to('cuda')\n",
    "\n",
    "inputs = [\n",
    "    \"vi: Lu·∫≠t n√†y kh√¥ng ƒëi·ªÅu ch·ªânh ho·∫°t ƒë·ªông nghi√™n c·ª©u v√† ph√°t tri·ªÉn h·ªá th·ªëng tr√≠ tu·ªá nh√¢n t·∫°o.\",\n",
    "    \"vi: Lu·∫≠t n√†y ƒëi·ªÅu ch·ªânh c√°c ho·∫°t ƒë·ªông cung c·∫•p, tri·ªÉn khai, s·ª≠ d·ª•ng h·ªá th·ªëng tr√≠ tu·ªá nh√¢n t·∫°o v√† quy ƒë·ªãnh quy·ªÅn, nghƒ©a v·ª• c·ªßa t·ªï ch·ª©c, c√° nh√¢n tham gia ho·∫°t ƒë·ªông tr√≠ tu·ªá nh√¢n t·∫°o t·∫°i Vi·ªát Nam.\",\n",
    "    \"vi: T·ªï ch·ª©c, c√° nh√¢n ƒë∆∞·ª£c quy·ªÅn t·ª± do kinh doanh m·ªçi ng√†nh, ngh·ªÅ m√† ph√°p lu·∫≠t kh√¥ng c·∫•m; ƒë·ªìng th·ªùi ƒë∆∞·ª£c quy·ªÅn m·ªü r·ªông quy m√¥ s·∫£n xu·∫•t ‚Äì kinh doanh m√† kh√¥ng b·ªã gi·ªõi h·∫°n v·ªÅ s·ªë l∆∞·ª£ng lao ƒë·ªông, ph·∫°m vi ho·∫°t ƒë·ªông v√† kh·ªëi l∆∞·ª£ng s·∫£n ph·∫©m, tr·ª´ tr∆∞·ªùng h·ª£p c√°c ho·∫°t ƒë·ªông ƒë√≥ g√¢y ·∫£nh h∆∞·ªüng nghi√™m tr·ªçng ƒë·∫øn qu·ªëc ph√≤ng, an ninh qu·ªëc gia ho·∫∑c m√¥i tr∆∞·ªùng sinh th√°i.\",\n",
    "    \"vi: M·ªçi t·ªï ch·ª©c, c√° nh√¢n trong n∆∞·ªõc ch·ªâ ƒë∆∞·ª£c ph√©p ti·∫øn h√†nh ho·∫°t ƒë·ªông s·∫£n xu·∫•t ‚Äì kinh doanh trong ph·∫°m vi ƒë·ªãa b√†n c·∫•p t·ªânh n∆°i ƒëƒÉng k√Ω kinh doanh ban ƒë·∫ßu, v√† tuy·ªát ƒë·ªëi kh√¥ng ƒë∆∞·ª£c m·ªü r·ªông quy m√¥ nh√¢n s·ª±, c∆° s·ªü v·∫≠n h√†nh ho·∫∑c tƒÉng kh·ªëi l∆∞·ª£ng s·∫£n ph·∫©m n·∫øu kh√¥ng c√≥ s·ª± ch·∫•p thu·∫≠n b·∫±ng vƒÉn b·∫£n c·ªßa c∆° quan qu·∫£n l√Ω chuy√™n ng√†nh c·∫•p trung ∆∞∆°ng.\"]\n",
    "\n",
    "outputs = model.generate(tokenizer(inputs, return_tensors=\"pt\", padding=True).input_ids.to('cuda'), max_length=512)\n",
    "translated_sentences = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "translated_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80beb2f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(chunks):\n",
    "    model_name = \"VietAI/envit5-translation\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to('cuda')\n",
    "\n",
    "    translated_sentences = []       \n",
    "    for sentences in chunks:\n",
    "        inputs = [f'vi: {sentence}' for sentence in sentences]\n",
    "        outputs = model.generate(tokenizer(inputs, return_tensors=\"pt\", padding=True).input_ids.to('cuda'), max_length=512)\n",
    "        outputs = [output[4:] for output in tokenizer.batch_decode(outputs, skip_special_tokens=True)]\n",
    "        translated_sentences.append(outputs)\n",
    "    return translated_sentences\n",
    "\n",
    "translated_sentences = translate(article_hierarchy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09bb8815",
   "metadata": {},
   "outputs": [],
   "source": [
    "for paragraph in paragraphs:\n",
    "    print(paragraph.text)\n",
    "    print(level_detector.get_level(paragraph.text))\n",
    "    print(\"-\"*100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232247fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "group = [\n",
    "    'ƒêi·ªÅu 1.',\n",
    "    '1. Kho·∫£n 1',\n",
    "    'a) ƒêi·ªÉm c',\n",
    "    'b) ƒêi·ªÉm b',\n",
    "    '',\n",
    "    'd) ƒêi·ªÉm d',\n",
    "    '2. Kho·∫£n 2',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86186b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "article = \"\"\n",
    "clause = \"\"\n",
    "point = \"\"\n",
    "sub_point = \"\"\n",
    "output = []\n",
    "\n",
    "for segment_idx, text_segment in enumerate(group):\n",
    "    curr_level = level_detector.get_level(text_segment)\n",
    "    curr_level_type = curr_level[\"type\"]\n",
    "    curr_level_id = curr_level[\"level_id\"]\n",
    "\n",
    "    # Look ahead to the next element's level (or 0 if this is the last one)\n",
    "    if segment_idx + 1 < len(group):\n",
    "        next_level = level_detector.get_level(group[segment_idx + 1])\n",
    "        next_level_id = next_level[\"level_id\"]\n",
    "    else:\n",
    "        next_level_id = 0  # sentinel: end of group ‚Üí force flush\n",
    "\n",
    "    # ------------------ ARTICLE (ƒêI·ªÄU) ------------------\n",
    "    if curr_level_type == \"ƒëi·ªÅu\":\n",
    "        # Reset lower-level context when a new article is found\n",
    "        article = text_segment\n",
    "        clause = \"\"\n",
    "        point = \"\"\n",
    "        sub_point = \"\"\n",
    "\n",
    "        # If there is no lower-level element after this article,\n",
    "        # we add the article itself.\n",
    "        if curr_level_id >= next_level_id:\n",
    "            output.append(article)\n",
    "\n",
    "    # ------------------ CLAUSE (KHO·∫¢N) ------------------\n",
    "    elif curr_level_type == \"kho·∫£n\":\n",
    "        clause = \"Kho·∫£n \" + text_segment\n",
    "        point = \"\"\n",
    "        sub_point = \"\"\n",
    "\n",
    "        # If no lower-level element follows (point/sub-point/content),\n",
    "        # we add the clause together with its article.\n",
    "        if curr_level_id >= next_level_id:\n",
    "            parts = [clause]\n",
    "            if article:\n",
    "                parts.append(article)\n",
    "            output.append(\" \".join(parts))\n",
    "\n",
    "    # ------------------ POINT (ƒêI·ªÇM) ------------------\n",
    "    elif curr_level_type == \"ƒëi·ªÉm\":\n",
    "        point = \"ƒêi·ªÉm \" + text_segment\n",
    "        sub_point = \"\"\n",
    "\n",
    "        if curr_level_id >= next_level_id:\n",
    "            parts = [point]\n",
    "            if clause:\n",
    "                parts.append(clause)\n",
    "            if article:\n",
    "                parts.append(article)\n",
    "            output.append(\" \".join(parts))\n",
    "\n",
    "    # ------------------ SUB-POINT (ƒêI·ªÇM CON) ------------------\n",
    "    elif curr_level_type == \"ƒëi·ªÉm con\":\n",
    "        sub_point = text_segment\n",
    "\n",
    "        if curr_level_id >= next_level_id:\n",
    "            parts = [sub_point]\n",
    "            if point:\n",
    "                parts.append(point)\n",
    "            if clause:\n",
    "                parts.append(clause)\n",
    "            if article:\n",
    "                parts.append(article)\n",
    "            output.append(\" \".join(parts))\n",
    "\n",
    "    # ------------------ CONTENT (N·ªòI DUNG) ------------------\n",
    "    elif curr_level_type == \"n·ªôi dung\":\n",
    "        # Lowest-level detailed content\n",
    "        if curr_level_id >= next_level_id:\n",
    "            parts = [text_segment]\n",
    "            if sub_point:\n",
    "                parts.append(sub_point)\n",
    "            if point:\n",
    "                parts.append(point)\n",
    "            if clause:\n",
    "                parts.append(clause)\n",
    "            if article:\n",
    "                parts.append(article)\n",
    "            output.append(\" \".join(parts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba1d9dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f4ecbac",
   "metadata": {},
   "outputs": [],
   "source": [
    "for articles in article_hierarchy:\n",
    "    for article in articles:\n",
    "        if \"d) Lu·∫≠t n√†y kh√¥ng ƒëi·ªÅu ch·ªânh ho·∫°t ƒë·ªông nghi√™n c·ª©u v√† ph√°t tri·ªÉn h·ªá th·ªëng tr√≠ tu·ªá nh√¢n t·∫°o.\" in article:\n",
    "            print(articles)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126de859",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"BatsResearch/bonito-experiment-eval\", \"contract_nli\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad44059c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "preds = []\n",
    "labels = []\n",
    "\n",
    "for row in tqdm(ds[\"test\"]):\n",
    "    prem = row[\"premise\"]\n",
    "    hypo = row[\"hypothesis\"]\n",
    "    out = pipe({\"text\": prem, \"text_pair\": hypo})\n",
    "\n",
    "    preds.append(out[\"label\"])\n",
    "    labels.append(row[\"label\"])\n",
    "\n",
    "map_label_to_int = {\"entailment\": 1, \"neutral\": 2, \"contradiction\": 3}\n",
    "pred_ids = [map_label_to_int[x] for x in preds]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b2bdc8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "acc = accuracy_score(labels, pred_ids)\n",
    "print(\"Accuracy =\", acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7fb556",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_path = \"/home/ducmb/fis_legal_nlp/F1/test.docx\"\n",
    "\n",
    "paragraphs_1 = docx.Document(temp_path).paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d87cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "translated_sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08ff4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = {}\n",
    "\n",
    "for pair in pairs:\n",
    "    article_1 = translated_sentences[pair[0]]\n",
    "    article_2 = translated_sentences[pair[1]]\n",
    "\n",
    "    for i in range(len(article_1)):\n",
    "        temp_1 = article_1[i]\n",
    "        pipe_input = []\n",
    "        for j in range(len(article_2)):\n",
    "            temp_2 = article_2[j]\n",
    "            pipe_input.append(dict(text=temp_1, text_pair=temp_2))\n",
    "        outs = pipe(pipe_input)\n",
    "        for index, out in enumerate(outs):\n",
    "            if out[\"label\"] == \"contradict\":\n",
    "                if output[temp_1] is None:\n",
    "                    output[temp_1] = article_2[index]\n",
    "                else:\n",
    "                    output[temp_1].append(article_2[index])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93b7248",
   "metadata": {},
   "outputs": [],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ada0a5b",
   "metadata": {},
   "source": [
    "#### Using LLM to extract sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a19edf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 2 files:   0%|          | 0/2 [12:59<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"Qwen/Qwen2.5-3B-Instruct\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "prompt = \"Give me a short introduction to large language model.\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "generated_ids = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=512\n",
    ")\n",
    "generated_ids = [\n",
    "    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "]\n",
    "\n",
    "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c78a259",
   "metadata": {},
   "source": [
    "#### Extract information in sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b19a93c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maibaduc/nlp/LegalChunk/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-11-26 21:16:57 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.11.0.json: 435kB [00:00, 10.3MB/s]                    \n",
      "2025-11-26 21:16:57 INFO: Downloaded file to /home/maibaduc/stanza_resources/resources.json\n",
      "Downloading https://huggingface.co/stanfordnlp/stanza-vi/resolve/v1.11.0/models/constituency/vlsp22_charlm.pt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 105M/105M [00:07<00:00, 13.4MB/s] \n",
      "Downloading https://huggingface.co/stanfordnlp/stanza-vi/resolve/v1.11.0/models/pretrain/conll17.pt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 107M/107M [00:19<00:00, 5.51MB/s] \n",
      "Downloading https://huggingface.co/stanfordnlp/stanza-vi/resolve/v1.11.0/models/forward_charlm/conll17.pt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 24.4M/24.4M [00:02<00:00, 11.1MB/s]\n",
      "Downloading https://huggingface.co/stanfordnlp/stanza-vi/resolve/v1.11.0/models/backward_charlm/conll17.pt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 24.4M/24.4M [00:02<00:00, 9.32MB/s]\n",
      "2025-11-26 21:17:35 INFO: Loading these models for language: vi (Vietnamese):\n",
      "================================\n",
      "| Processor    | Package       |\n",
      "--------------------------------\n",
      "| tokenize     | vtb           |\n",
      "| pos          | vtb_charlm    |\n",
      "| constituency | vlsp22_charlm |\n",
      "================================\n",
      "\n",
      "2025-11-26 21:17:37 INFO: Using device: cuda\n",
      "2025-11-26 21:17:37 INFO: Loading: tokenize\n",
      "2025-11-26 21:17:39 INFO: Loading: pos\n",
      "2025-11-26 21:17:40 INFO: Loading: constituency\n",
      "2025-11-26 21:17:41 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(ROOT (S (NP (N Vi·ªác) (SBAR (S (NP (Det nh·ªØng) (N c√¥ng ngh·ªá) (Adj m·ªõi)) (VP (VP (Adj li√™n t·ª•c) (V xu·∫•t hi·ªán)) (CC v√†) (VP (V t·∫°o) (Adv ra) (NP (Det nh·ªØng) (N thay ƒë·ªïi) (AP (Adj kh√≥) (V l∆∞·ªùng)) (PP (Pre trong) (NP (N ƒë·ªùi s·ªëng) (N x√£ h·ªôi))))))))) (VP (V ƒë·∫∑t) (Adv ra) (NP (V y√™u c·∫ßu) (SBAR (SC r·∫±ng) (S (S (NP (Det c√°c) (N c∆° quan) (V qu·∫£n l√Ω)) (, ,) (SC d√π) (VP (AUX ph·∫£i) (VP (V ƒë·ªëi m·∫∑t) (PP (Pre v·ªõi) (NP (Adj nhi·ªÅu) (N h·∫°n ch·∫ø) (PP (Pre v·ªÅ) (NP (N ngu·ªìn l·ª±c)))))))) (, ,) (VP (Adv v·∫´n) (V c·∫ßn) (VP (V x√¢y d·ª±ng) (NP (Det nh·ªØng) (N khung) (N ph√°p l√Ω) (AP (Adj ƒë·ªß) (Adj linh ho·∫°t)) (PP (Pre ƒë·ªÉ) (VP (V b·∫£o ƒë·∫£m) (NP (N quy·ªÅn) (CC v√†) (N l·ª£i √≠ch) (AP (Adj ch√≠nh ƒë√°ng)) (PP (Pre c·ªßa) (NP (N ng∆∞·ªùi) (N d√¢n)))))))))))))))\n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "\n",
    "nlp = stanza.Pipeline(lang='vi', processors='tokenize,pos,constituency')\n",
    "doc = nlp('Vi·ªác nh·ªØng c√¥ng ngh·ªá m·ªõi li√™n t·ª•c xu·∫•t hi·ªán v√† t·∫°o ra nh·ªØng thay ƒë·ªïi kh√≥ l∆∞·ªùng trong ƒë·ªùi s·ªëng x√£ h·ªôi ƒë·∫∑t ra y√™u c·∫ßu r·∫±ng c√°c c∆° quan qu·∫£n l√Ω, d√π ph·∫£i ƒë·ªëi m·∫∑t v·ªõi nhi·ªÅu h·∫°n ch·∫ø v·ªÅ ngu·ªìn l·ª±c, v·∫´n c·∫ßn x√¢y d·ª±ng nh·ªØng khung ph√°p l√Ω ƒë·ªß linh ho·∫°t ƒë·ªÉ b·∫£o ƒë·∫£m quy·ªÅn v√† l·ª£i √≠ch ch√≠nh ƒë√°ng c·ªßa ng∆∞·ªùi d√¢n')\n",
    "for sentence in doc.sentences:\n",
    "    print(sentence.constituency)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "395c3ffb",
   "metadata": {},
   "source": [
    "(ROOT (S (NP (N Vi·ªác) (SBAR (S (NP (Det nh·ªØng) (N c√¥ng ngh·ªá) (Adj m·ªõi)) (VP (VP (Adj li√™n t·ª•c) (V xu·∫•t hi·ªán)) (CC v√†) (VP (V t·∫°o) (Adv ra) (NP (Det nh·ªØng) (N thay ƒë·ªïi) (AP (Adj kh√≥) (V l∆∞·ªùng)) (PP (Pre trong) (NP (N ƒë·ªùi s·ªëng) (N x√£ h·ªôi))))))))) (VP (V ƒë·∫∑t) (Adv ra) (NP (V y√™u c·∫ßu) (SBAR (SC r·∫±ng) (S (S (NP (Det c√°c) (N c∆° quan) (V qu·∫£n l√Ω)) (, ,) (SC d√π) (VP (AUX ph·∫£i) (VP (V ƒë·ªëi m·∫∑t) (PP (Pre v·ªõi) (NP (Adj nhi·ªÅu) (N h·∫°n ch·∫ø) (PP (Pre v·ªÅ) (NP (N ngu·ªìn l·ª±c)))))))) (, ,) (VP (Adv v·∫´n) (V c·∫ßn) (VP (V x√¢y d·ª±ng) (NP (Det nh·ªØng) (N khung) (N ph√°p l√Ω) (AP (Adj ƒë·ªß) (Adj linh ho·∫°t)) (PP (Pre ƒë·ªÉ) (VP (V b·∫£o ƒë·∫£m) (NP (N quy·ªÅn) (CC v√†) (N l·ª£i √≠ch) (AP (Adj ch√≠nh ƒë√°ng)) (PP (Pre c·ªßa) (NP (N ng∆∞·ªùi) (N d√¢n)))))))))))))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf8b033b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(ROOT (S (NP (N C∆° s·ªü) (VP (V kh√°m ch·ªØa) (NP (N b·ªánh)))) (VP (AUX ph·∫£i) (VP (V thu ph√≠) (VP (V kh√°m b·ªánh) (PP (Pre ƒë·ªëi v·ªõi) (NP (N ng∆∞·ªùi) (VP (Adv ch∆∞a) (V c√≥) (NP (N th·∫ª) (V b·∫£o hi·ªÉm y t·∫ø)))))))) (. .)))\n"
     ]
    }
   ],
   "source": [
    "doc = nlp('C∆° s·ªü kh√°m ch·ªØa b·ªánh ph·∫£i thu ph√≠ kh√°m b·ªánh ƒë·ªëi v·ªõi ng∆∞·ªùi ch∆∞a c√≥ th·∫ª b·∫£o hi·ªÉm y t·∫ø.')\n",
    "for sentence in doc.sentences:\n",
    "    print(sentence.constituency)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdfdf8df",
   "metadata": {},
   "source": [
    "(ROOT (S (NP (N C∆° s·ªü) (VP (V kh√°m ch·ªØa) (NP (N b·ªánh)))) (VP (AUX ph·∫£i) (VP (V thu ph√≠) (VP (V kh√°m b·ªánh) (PP (Pre ƒë·ªëi v·ªõi) (NP (N ng∆∞·ªùi) (VP (Adv ch∆∞a) (V c√≥) (NP (N th·∫ª) (V b·∫£o hi·ªÉm y t·∫ø)))))))) (. .)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5c7a239f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "(VP (V ƒêƒÉng k√Ω) (NP (N doanh nghi·ªáp)))\n",
      "=== SUBJECT (NP) ===\n",
      "None\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'is_leaf'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[51], line 33\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28mprint\u001b[39m(predicate_tree)\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=== SUBJECT (NP) ===\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 33\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mflatten_tree\u001b[49m\u001b[43m(\u001b[49m\u001b[43msubject_tree\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m=== PREDICATE (VP) ===\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28mprint\u001b[39m(flatten_tree(predicate_tree))\n",
      "Cell \u001b[0;32mIn[51], line 16\u001b[0m, in \u001b[0;36mflatten_tree\u001b[0;34m(tree)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Chuy·ªÉn c√¢y th√†nh chu·ªói text\"\"\"\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(tree)\n\u001b[0;32m---> 16\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mtree\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_leaf\u001b[49m():\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tree\u001b[38;5;241m.\u001b[39mlabel\n\u001b[1;32m     18\u001b[0m parts \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'is_leaf'"
     ]
    }
   ],
   "source": [
    "def get_np_vp(tree):\n",
    "    subject = None\n",
    "    predicate = None\n",
    "    # duy·ªát c√°c con c·ªßa node S\n",
    "    for child in tree.children:\n",
    "        label = child.label\n",
    "        if label == \"NP\" and subject is None:\n",
    "            subject = child\n",
    "        elif label == \"VP\" and predicate is None:\n",
    "            predicate = child\n",
    "    return subject, predicate\n",
    "\n",
    "def flatten_tree(tree):\n",
    "    \"\"\"Chuy·ªÉn c√¢y th√†nh chu·ªói text\"\"\"\n",
    "    print(tree)\n",
    "    if tree.is_leaf():\n",
    "        return tree.label\n",
    "    parts = []\n",
    "    for child in tree.children:\n",
    "        parts.append(flatten_tree(child))\n",
    "    return \" \".join(parts)\n",
    "\n",
    "# L·∫•y c√¢u ƒë·∫ßu ti√™n\n",
    "sent = doc.sentences[0]\n",
    "tree = sent.constituency\n",
    "\n",
    "# extract subject + predicate\n",
    "subject_tree, predicate_tree = get_np_vp(tree.children[0])  # ROOT -> S\n",
    "print(subject_tree)\n",
    "print(predicate_tree)\n",
    "\n",
    "print(\"=== SUBJECT (NP) ===\")\n",
    "print(flatten_tree(subject_tree))\n",
    "\n",
    "print(\"\\n=== PREDICATE (VP) ===\")\n",
    "print(flatten_tree(predicate_tree))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "fbe38621",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(NP (N C·ªïng) (N th√¥ng tin) (N ƒëi·ªán t·ª≠) (NP (Num m·ªôt) (N c·ª≠a) (PP (V v·ªÅ) (NP (N tr√≠ tu·ªá nh√¢n t·∫°o)))))\n",
      "NP\n",
      "(VP (VP (V l√†) (NP (N h·ªá th·ªëng) (N th√†nh ph·∫ßn) (PP (Pre c·ªßa) (NP (N C·ªïng) (N d·ªãch v·ª•) (NP (N c√¥ng qu·ªëc) (Nb gia)))))) (, ,) (VP (V th·ª±c hi·ªán) (VP (V ti·∫øp nh·∫≠n) (, ,) (V x·ª≠ l√Ω) (NP (N h·ªì s∆°)))) (, ,) (VP (V cung c·∫•p) (NP (N d·ªãch v·ª•) (N c√¥ng) (AP (Adj tr·ª±c tuy·∫øn)))) (CC v√†) (VP (Adj c√¥ng khai) (NP (N th√¥ng tin)) (VP (Pre theo) (NP (N quy ƒë·ªãnh) (PP (Pre c·ªßa) (NP (N ph√°p lu·∫≠t)))))))\n",
      "VP\n",
      "(. .)\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "text = '''\n",
    "C·ªïng th√¥ng tin ƒëi·ªán t·ª≠ m·ªôt c·ª≠a v·ªÅ tr√≠ tu·ªá nh√¢n t·∫°o l√† h·ªá th·ªëng th√†nh ph·∫ßn c·ªßa C·ªïng d·ªãch v·ª• c√¥ng qu·ªëc gia, th·ª±c hi·ªán ti·∫øp nh·∫≠n, x·ª≠ l√Ω h·ªì s∆°, cung c·∫•p d·ªãch v·ª• c√¥ng tr·ª±c tuy·∫øn v√† c√¥ng khai th√¥ng tin theo quy ƒë·ªãnh c·ªßa ph√°p lu·∫≠t.\n",
    "'''\n",
    "\n",
    "doc = nlp(text)\n",
    "\n",
    "tree = doc.sentences[0].constituency\n",
    "\n",
    "import re\n",
    "\n",
    "ignore_sentence = [\"Aux\", \"Num\", \"\"]\n",
    "\n",
    "tmp = tree.children[0]\n",
    "for child in tmp.children:\n",
    "    print(child)\n",
    "    print(child.label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd52878",
   "metadata": {},
   "source": [
    "(NP (NP (N Tr∆∞·ªùng h·ª£p) (SBAR (S (NP (N c√¥ng ty) (VP (N tr√°ch nhi·ªám) (NP (V h·ªØu h·∫°n) (CC v√†) (N c√¥ng ty c·ªï ph·∫ßn)))) (VP (V c√≥) (AP (Adj nhi·ªÅu) (Adv h∆°n)))))) (CC v√†) (NP (Num m·ªôt) (N ng∆∞·ªùi) (VP (V ƒë·∫°i di·ªán) (VP (Pre theo) (NP (N ph√°p lu·∫≠t))))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b526165c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-26 21:30:20 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.11.0.json: 435kB [00:00, 3.68MB/s]                    \n",
      "2025-11-26 21:30:20 INFO: Downloaded file to /home/maibaduc/stanza_resources/resources.json\n",
      "2025-11-26 21:30:21 INFO: Loading these models for language: vi (Vietnamese):\n",
      "================================\n",
      "| Processor    | Package       |\n",
      "--------------------------------\n",
      "| tokenize     | vtb           |\n",
      "| pos          | vtb_charlm    |\n",
      "| constituency | vlsp22_charlm |\n",
      "================================\n",
      "\n",
      "2025-11-26 21:30:21 INFO: Using device: cuda\n",
      "2025-11-26 21:30:21 INFO: Loading: tokenize\n",
      "2025-11-26 21:30:21 INFO: Loading: pos\n",
      "2025-11-26 21:30:23 INFO: Loading: constituency\n",
      "2025-11-26 21:30:24 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "import json\n",
    "from stanza.models.constituency.parse_tree import Tree\n",
    "\n",
    "nlp = stanza.Pipeline(lang='vi', processors='tokenize,pos,constituency')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "57580457",
   "metadata": {},
   "outputs": [],
   "source": [
    "def constituency_tree_to_json(tree: Tree):\n",
    "    def _convert(node: Tree):\n",
    "        # L√°: ch√≠nh l√† t·ª´\n",
    "        if node.is_leaf():\n",
    "            return {\n",
    "                \"type\": \"leaf\",\n",
    "                \"word\": node.label\n",
    "            }\n",
    "\n",
    "        # Preterminal: POS tag + 1 con l√† t·ª´\n",
    "        if node.is_preterminal():\n",
    "            return {\n",
    "                \"pos\": node.label,\n",
    "                \"word\": node.children[0].label\n",
    "            }\n",
    "\n",
    "        # N√∫t n·ªôi b·ªô: NP, VP, S, ...\n",
    "        return {\n",
    "            \"label\": node.label,\n",
    "            \"children\": [_convert(child) for child in node.children]\n",
    "        }\n",
    "\n",
    "    return _convert(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "84b57cfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"label\": \"ROOT\",\n",
      "  \"children\": [\n",
      "    {\n",
      "      \"label\": \"S\",\n",
      "      \"children\": [\n",
      "        {\n",
      "          \"label\": \"NP\",\n",
      "          \"children\": [\n",
      "            {\n",
      "              \"pos\": \"Det\",\n",
      "              \"word\": \"Nh·ªØng\"\n",
      "            },\n",
      "            {\n",
      "              \"pos\": \"N\",\n",
      "              \"word\": \"ng∆∞·ªùi\"\n",
      "            },\n",
      "            {\n",
      "              \"label\": \"VP\",\n",
      "              \"children\": [\n",
      "                {\n",
      "                  \"label\": \"VP\",\n",
      "                  \"children\": [\n",
      "                    {\n",
      "                      \"pos\": \"V\",\n",
      "                      \"word\": \"l√†m vi·ªác\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"label\": \"PP\",\n",
      "                      \"children\": [\n",
      "                        {\n",
      "                          \"pos\": \"Pre\",\n",
      "                          \"word\": \"trong\"\n",
      "                        },\n",
      "                        {\n",
      "                          \"label\": \"NP\",\n",
      "                          \"children\": [\n",
      "                            {\n",
      "                              \"pos\": \"N\",\n",
      "                              \"word\": \"ƒëi·ªÅu ki·ªán\"\n",
      "                            },\n",
      "                            {\n",
      "                              \"label\": \"AP\",\n",
      "                              \"children\": [\n",
      "                                {\n",
      "                                  \"pos\": \"Adj\",\n",
      "                                  \"word\": \"b√¨nh th∆∞·ªùng\"\n",
      "                                }\n",
      "                              ]\n",
      "                            }\n",
      "                          ]\n",
      "                        }\n",
      "                      ]\n",
      "                    }\n",
      "                  ]\n",
      "                },\n",
      "                {\n",
      "                  \"pos\": \"CC\",\n",
      "                  \"word\": \"v√†\"\n",
      "                },\n",
      "                {\n",
      "                  \"label\": \"VP\",\n",
      "                  \"children\": [\n",
      "                    {\n",
      "                      \"pos\": \"V\",\n",
      "                      \"word\": \"c√≥\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"label\": \"NP\",\n",
      "                      \"children\": [\n",
      "                        {\n",
      "                          \"pos\": \"N\",\n",
      "                          \"word\": \"th·ªùi gian\"\n",
      "                        },\n",
      "                        {\n",
      "                          \"label\": \"VP\",\n",
      "                          \"children\": [\n",
      "                            {\n",
      "                              \"pos\": \"V\",\n",
      "                              \"word\": \"ƒë√≥ng\"\n",
      "                            },\n",
      "                            {\n",
      "                              \"label\": \"NP\",\n",
      "                              \"children\": [\n",
      "                                {\n",
      "                                  \"pos\": \"N\",\n",
      "                                  \"word\": \"b·∫£o hi·ªÉm\"\n",
      "                                },\n",
      "                                {\n",
      "                                  \"pos\": \"N\",\n",
      "                                  \"word\": \"x√£ h·ªôi\"\n",
      "                                }\n",
      "                              ]\n",
      "                            }\n",
      "                          ]\n",
      "                        }\n",
      "                      ]\n",
      "                    }\n",
      "                  ]\n",
      "                },\n",
      "                {\n",
      "                  \"label\": \"PP\",\n",
      "                  \"children\": [\n",
      "                    {\n",
      "                      \"pos\": \"Pre\",\n",
      "                      \"word\": \"t·ª´\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"label\": \"AP\",\n",
      "                      \"children\": [\n",
      "                        {\n",
      "                          \"pos\": \"Adj\",\n",
      "                          \"word\": \"ƒë·ªß\"\n",
      "                        },\n",
      "                        {\n",
      "                          \"label\": \"NP\",\n",
      "                          \"children\": [\n",
      "                            {\n",
      "                              \"pos\": \"Num\",\n",
      "                              \"word\": \"30\"\n",
      "                            },\n",
      "                            {\n",
      "                              \"pos\": \"N\",\n",
      "                              \"word\": \"nƒÉm\"\n",
      "                            },\n",
      "                            {\n",
      "                              \"label\": \"VP\",\n",
      "                              \"children\": [\n",
      "                                {\n",
      "                                  \"pos\": \"V\",\n",
      "                                  \"word\": \"tr·ªü\"\n",
      "                                },\n",
      "                                {\n",
      "                                  \"pos\": \"V\",\n",
      "                                  \"word\": \"l√™n\"\n",
      "                                }\n",
      "                              ]\n",
      "                            }\n",
      "                          ]\n",
      "                        }\n",
      "                      ]\n",
      "                    }\n",
      "                  ]\n",
      "                }\n",
      "              ]\n",
      "            }\n",
      "          ]\n",
      "        },\n",
      "        {\n",
      "          \"pos\": \",\",\n",
      "          \"word\": \",\"\n",
      "        },\n",
      "        {\n",
      "          \"label\": \"NP\",\n",
      "          \"children\": [\n",
      "            {\n",
      "              \"pos\": \"Num\",\n",
      "              \"word\": \"m·ªói\"\n",
      "            },\n",
      "            {\n",
      "              \"pos\": \"N\",\n",
      "              \"word\": \"nƒÉm\"\n",
      "            }\n",
      "          ]\n",
      "        },\n",
      "        {\n",
      "          \"label\": \"VP\",\n",
      "          \"children\": [\n",
      "            {\n",
      "              \"pos\": \"V\",\n",
      "              \"word\": \"c√≥\"\n",
      "            },\n",
      "            {\n",
      "              \"label\": \"SBAR\",\n",
      "              \"children\": [\n",
      "                {\n",
      "                  \"label\": \"S\",\n",
      "                  \"children\": [\n",
      "                    {\n",
      "                      \"label\": \"NP\",\n",
      "                      \"children\": [\n",
      "                        {\n",
      "                          \"pos\": \"N\",\n",
      "                          \"word\": \"t·ªïng s·ªë\"\n",
      "                        },\n",
      "                        {\n",
      "                          \"pos\": \"N\",\n",
      "                          \"word\": \"ng√†y\"\n",
      "                        },\n",
      "                        {\n",
      "                          \"label\": \"VP\",\n",
      "                          \"children\": [\n",
      "                            {\n",
      "                              \"pos\": \"V\",\n",
      "                              \"word\": \"ngh·ªâ\"\n",
      "                            },\n",
      "                            {\n",
      "                              \"pos\": \"V\",\n",
      "                              \"word\": \"l√†m vi·ªác\"\n",
      "                            }\n",
      "                          ]\n",
      "                        }\n",
      "                      ]\n",
      "                    },\n",
      "                    {\n",
      "                      \"label\": \"VP\",\n",
      "                      \"children\": [\n",
      "                        {\n",
      "                          \"pos\": \"SC\",\n",
      "                          \"word\": \"l√†\"\n",
      "                        },\n",
      "                        {\n",
      "                          \"label\": \"NP\",\n",
      "                          \"children\": [\n",
      "                            {\n",
      "                              \"pos\": \"Num\",\n",
      "                              \"word\": \"60\"\n",
      "                            },\n",
      "                            {\n",
      "                              \"pos\": \"N\",\n",
      "                              \"word\": \"ng√†y\"\n",
      "                            }\n",
      "                          ]\n",
      "                        }\n",
      "                      ]\n",
      "                    }\n",
      "                  ]\n",
      "                }\n",
      "              ]\n",
      "            }\n",
      "          ]\n",
      "        },\n",
      "        {\n",
      "          \"pos\": \".\",\n",
      "          \"word\": \".\"\n",
      "        }\n",
      "      ]\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "text = (\n",
    "    \"Nh·ªØng ng∆∞·ªùi l√†m vi·ªác trong ƒëi·ªÅu ki·ªán b√¨nh th∆∞·ªùng v√† c√≥ th·ªùi gian ƒë√≥ng b·∫£o hi·ªÉm x√£ h·ªôi t·ª´ ƒë·ªß 30 nƒÉm tr·ªü l√™n, m·ªói nƒÉm c√≥ t·ªïng s·ªë ng√†y ngh·ªâ l√†m vi·ªác l√† 60 ng√†y.\"\n",
    ")\n",
    "\n",
    "doc = nlp(text)\n",
    "\n",
    "for sent in doc.sentences:\n",
    "    tree = sent.constituency          # ƒë√¢y l√† m·ªôt ƒë·ªëi t∆∞·ª£ng Tree t·ª´ parse_tree.py\n",
    "    tree_json = constituency_tree_to_json(tree)\n",
    "    print(json.dumps(tree_json, ensure_ascii=False, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7961ee17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ch·ª©c danh', 'ph·ª• c·∫•p l∆∞∆°ng', 'c√°n b·ªô', 'c√¥ng ch·ª©c', 'vi√™n ch·ª©c', 's·ªë 128-Qƒê/TW', 'nƒÉm 2004', 'Trung ∆∞∆°ng ƒê·∫£ng', 'c√°n b·ªô', 'c√¥ng ch·ª©c', 'c∆° quan ƒê·∫£ng', 'M·∫∑t tr·∫≠n', 'c√°c ƒëo√†n th·ªÉ', 's·ªë 204/2004/Nƒê-CP', 'nƒÉm 2004', 'Ch√≠nh ph·ªß', 'c√°n b·ªô', 'c√¥ng ch·ª©c', 'vi√™n ch·ª©c', 'l·ª±c l∆∞·ª£ng v≈© trang', 'sau ƒë√¢y', 's·ªë 204/2004/Nƒê-CP', 's·ªë 76/2009/Nƒê-CP', 'nƒÉm 2009', 'Ch√≠nh ph·ªß', 's·ªë 204/2004/Nƒê-CP', 's·ªë 17/2013/Nƒê-CP', 'nƒÉm 2013', 'Ch√≠nh ph·ªß', 's·ªë 204/2004/Nƒê-CP;', 's·ªë 92/2009/Nƒê-CP', 'nƒÉm 2009', 'Ch√≠nh ph·ªß', 'ch·ª©c danh , s·ªë l∆∞·ª£ng', 'c√°n b·ªô', 'c√¥ng ch·ª©c', 'x√£', 'ph∆∞·ªùng', 'th·ªã tr·∫•n', 'kh√¥ng chuy√™n tr√°ch', 's·ªë 730/2004/NQ -UBTVQH11', 'nƒÉm 2004', 'Th∆∞·ªùng v·ª• Qu·ªëc h·ªôi', 'b·∫£ng l∆∞∆°ng ch·ª©c v·ª•', 'ch·ª©c v·ª•', 'Nh√† n∆∞·ªõc', 'b·∫£ng l∆∞∆°ng chuy√™n m√¥n', 'ng√†nh T√≤a √°n', 'ng√†nh Ki·ªÉm s√°t', 's·ªë 823/2009/UBTVQH', 'nƒÉm 2009', 'Th∆∞·ªùng v·ª• Qu·ªëc h·ªôi', 's·ªë 1003/2006/NQ-UBTVQH11', 'nƒÉm 2006', 'Th∆∞·ªùng v·ª• Qu·ªëc h·ªôi', 'b·∫£ng l∆∞∆°ng', 'ch·ª©c v·ª•', 'Ki·ªÉm to√°n Nh√† n∆∞·ªõc', 'b·∫£ng l∆∞∆°ng', 'trang ph·ª•c', 'c√°n b·ªô', 'c√¥ng ch·ª©c', 'Nh√† n∆∞·ªõc', 'nh√† n∆∞·ªõc']\n"
     ]
    }
   ],
   "source": [
    "def get_base_noun_phrases(tree: Tree):\n",
    "    \"\"\"\n",
    "    Tr·∫£ v·ªÅ list c√°c c·ª•m danh t·ª´ \"g·ªëc\" (base NP):\n",
    "    - L√† c√°c node c√≥ label == 'NP'\n",
    "    - B√™n d∆∞·ªõi KH√îNG c√≤n node NP n√†o n·ªØa\n",
    "    \"\"\"\n",
    "\n",
    "    noun_phrases = []\n",
    "\n",
    "    def collect_leaves(node: Tree):\n",
    "        \"\"\"L·∫•y to√†n b·ªô t·ª´ (leaf) d∆∞·ªõi m·ªôt node.\"\"\"\n",
    "        words = []\n",
    "\n",
    "        def dfs(n: Tree):\n",
    "            if n.is_leaf():\n",
    "                words.append(n.label)\n",
    "            else:\n",
    "                for c in n.children:\n",
    "                    dfs(c)\n",
    "\n",
    "        dfs(node)\n",
    "        return \" \".join(words)\n",
    "\n",
    "    def has_np_descendant(node: Tree):\n",
    "        \"\"\"Ki·ªÉm tra trong subtree c·ªßa node c√≥ NP con n√†o kh√¥ng (tr·ª´ node hi·ªán t·∫°i).\"\"\"\n",
    "        for c in node.children:\n",
    "            if c.label == \"NP\":\n",
    "                return True\n",
    "            if has_np_descendant(c):\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def visit(node: Tree):\n",
    "        # N·∫øu l√† NP v√† b√™n d∆∞·ªõi kh√¥ng c√≤n NP n·ªØa -> base NP\n",
    "        if node.label == \"NP\" and not has_np_descendant(node):\n",
    "            phrase = collect_leaves(node)\n",
    "            noun_phrases.append(phrase)\n",
    "\n",
    "        # Duy·ªát ti·∫øp c√°c con (ƒë·ªÉ t√¨m NP kh√°c)\n",
    "        for c in node.children:\n",
    "            visit(c)\n",
    "\n",
    "    visit(tree)\n",
    "    return noun_phrases\n",
    "\n",
    "text = (\n",
    "    \"H·ªá s·ªë ti·ªÅn l∆∞∆°ng theo ng·∫°ch, b·∫≠c, ch·ª©c v·ª•, ch·ª©c danh v√† ph·ª• c·∫•p l∆∞∆°ng ƒë·ªëi v·ªõi c√°n b·ªô, c√¥ng ch·ª©c, vi√™n ch·ª©c ƒë∆∞·ª£c quy ƒë·ªãnh t·∫°i c√°c vƒÉn b·∫£n: Quy·∫øt ƒë·ªãnh s·ªë 128-Qƒê/TW ng√†y 14 th√°ng 12 nƒÉm 2004 c·ªßa Ban B√≠ th∆∞ Trung ∆∞∆°ng ƒê·∫£ng v·ªÅ ch·∫ø ƒë·ªô ti·ªÅn l∆∞∆°ng ƒë·ªëi v·ªõi c√°n b·ªô, c√¥ng ch·ª©c, vi√™n ch·ª©c c∆° quan ƒê·∫£ng, M·∫∑t tr·∫≠n v√† c√°c ƒëo√†n th·ªÉ; Ngh·ªã ƒë·ªãnh s·ªë 204/2004/Nƒê-CP ng√†y 14 th√°ng 12 nƒÉm 2004 c·ªßa Ch√≠nh ph·ªß v·ªÅ ch·∫ø ƒë·ªô ti·ªÅn l∆∞∆°ng ƒë·ªëi v·ªõi c√°n b·ªô, c√¥ng ch·ª©c, vi√™n ch·ª©c v√† l·ª±c l∆∞·ª£ng v≈© trang (sau ƒë√¢y g·ªçi t·∫Øt l√† Ngh·ªã ƒë·ªãnh s·ªë 204/2004/Nƒê-CP); Ngh·ªã ƒë·ªãnh s·ªë 76/2009/Nƒê-CP ng√†y 15 th√°ng 9 nƒÉm 2009 c·ªßa Ch√≠nh ph·ªß s·ª≠a ƒë·ªïi, b·ªï sung m·ªôt s·ªë ƒëi·ªÅu c·ªßa Ngh·ªã ƒë·ªãnh s·ªë 204/2004/Nƒê-CP v√† Ngh·ªã ƒë·ªãnh s·ªë 17/2013/Nƒê-CP ng√†y 19 th√°ng 02 nƒÉm 2013 c·ªßa Ch√≠nh ph·ªß s·ª≠a ƒë·ªïi, b·ªï sung m·ªôt s·ªë ƒêi·ªÅu c·ªßa Ngh·ªã ƒë·ªãnh s·ªë 204/2004/Nƒê-CP; Ngh·ªã ƒë·ªãnh s·ªë 92/2009/Nƒê-CP ng√†y 22 th√°ng 10 nƒÉm 2009 c·ªßa Ch√≠nh ph·ªß v·ªÅ ch·ª©c danh, s·ªë l∆∞·ª£ng, m·ªôt s·ªë ch·∫ø ƒë·ªô, ch√≠nh s√°ch ƒë·ªëi v·ªõi c√°n b·ªô, c√¥ng ch·ª©c ·ªü x√£, ph∆∞·ªùng, th·ªã tr·∫•n v√† nh·ªØng ng∆∞·ªùi ho·∫°t ƒë·ªông kh√¥ng chuy√™n tr√°ch ·ªü c·∫•p x√£; Ngh·ªã quy·∫øt s·ªë 730/2004/NQ-UBTVQH11 ng√†y 30 th√°ng 9 nƒÉm 2004 c·ªßa ·ª¶y ban Th∆∞·ªùng v·ª• Qu·ªëc h·ªôi v·ªÅ vi·ªác ph√™ chu·∫©n b·∫£ng l∆∞∆°ng ch·ª©c v·ª•, b·∫£ng ph·ª• c·∫•p ch·ª©c v·ª• ƒë·ªëi v·ªõi c√°n b·ªô l√£nh ƒë·∫°o c·ªßa Nh√† n∆∞·ªõc; b·∫£ng l∆∞∆°ng chuy√™n m√¥n, nghi·ªáp v·ª• ng√†nh T√≤a √°n, ng√†nh Ki·ªÉm s√°t; Ngh·ªã quy·∫øt s·ªë 823/2009/UBTVQH ng√†y 03 th√°ng 10 nƒÉm 2009 c·ªßa ·ª¶y ban Th∆∞·ªùng v·ª• Qu·ªëc h·ªôi s·ª≠a ƒë·ªïi, b·ªï sung m·ªôt s·ªë ƒêi·ªÅu c·ªßa Ngh·ªã quy·∫øt s·ªë 730/2004/NQ-UBTVQH11; Ngh·ªã quy·∫øt s·ªë 1003/2006/NQ-UBTVQH11 ng√†y 03 th√°ng 3 nƒÉm 2006 c·ªßa ·ª¶y ban Th∆∞·ªùng v·ª• Qu·ªëc h·ªôi ph√™ chu·∫©n b·∫£ng l∆∞∆°ng v√† ph·ª• c·∫•p ch·ª©c v·ª• ƒë·ªëi v·ªõi c√°n b·ªô l√£nh ƒë·∫°o Ki·ªÉm to√°n Nh√† n∆∞·ªõc; b·∫£ng l∆∞∆°ng, ph·ª• c·∫•p trang ph·ª•c ƒë·ªëi v·ªõi c√°n b·ªô, c√¥ng ch·ª©c Ki·ªÉm to√°n Nh√† n∆∞·ªõc, ch·∫ø ƒë·ªô ∆∞u ti√™n ƒë·ªëi v·ªõi Ki·ªÉm to√°n vi√™n nh√† n∆∞·ªõc.\"\n",
    ")\n",
    "\n",
    "doc = nlp(text)\n",
    "\n",
    "for sent in doc.sentences:\n",
    "    tree = sent.constituency\n",
    "    base_nps = get_base_noun_phrases(tree)\n",
    "    print(base_nps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "caea57e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clause 1:\n",
      "  Ch·ªß ng·ªØ : C√°c B·ªô , ng√†nh , ƒë·ªãa ph∆∞∆°ng\n",
      "  V·ªã ng·ªØ  : ch·ªâ ƒë·∫°o , t·ªï ch·ª©c r√† so√°t , ki·ªÉm tra vi·ªác qu·∫£n l√Ω v√† s·ª≠ d·ª•ng bi√™n ch·∫ø t·∫°i c√°c c∆° quan , t·ªï ch·ª©c , ƒë∆°n v·ªã s·ª± nghi·ªáp c√¥ng l·∫≠p thu·ªôc v√† tr·ª±c thu·ªôc ; c√≥ ph∆∞∆°ng √°n b·ªë tr√≠ , s·∫Øp x·∫øp bi√™n ch·∫ø s·ª≠ d·ª•ng v∆∞·ª£t so v·ªõi quy ƒë·ªãnh t·∫°i c√°c c∆° quan , t·ªï ch·ª©c , ƒë∆°n v·ªã s∆∞ nghi·ªáp c√¥ng l·∫≠p .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "from stanza.models.constituency.parse_tree import Tree\n",
    "\n",
    "# L·∫•y to√†n b·ªô t·ª´ (leaf) d∆∞·ªõi m·ªôt node\n",
    "def node_text(node: Tree) -> str:\n",
    "    return \" \".join(node.leaf_labels())\n",
    "\n",
    "# T√¨m t·∫•t c·∫£ c√°c n√∫t S (m·ªói S ~ m·ªôt m·ªánh ƒë·ªÅ)\n",
    "def find_clauses(root: Tree):\n",
    "    clauses = []\n",
    "\n",
    "    def dfs(n: Tree):\n",
    "        if n.label == \"S\":\n",
    "            clauses.append(n)\n",
    "        for c in n.children:\n",
    "            if isinstance(c, Tree) and not c.is_leaf():\n",
    "                dfs(c)\n",
    "\n",
    "    dfs(root)\n",
    "    return clauses\n",
    "\n",
    "# T√°ch 1 n√∫t S th√†nh ch·ªß ng·ªØ ‚Äì v·ªã ng·ªØ (heuristic)\n",
    "def split_subject_predicate(s_node: Tree):\n",
    "    # B·ªè qua c√°c leaf tr·ª±c ti·∫øp (th∆∞·ªùng kh√¥ng c√≥)\n",
    "    children = [c for c in s_node.children if isinstance(c, Tree)]\n",
    "\n",
    "    subject_nodes = []\n",
    "    predicate_nodes = []\n",
    "\n",
    "    seen_verb = False\n",
    "\n",
    "    for c in children:\n",
    "        label = c.label or \"\"\n",
    "\n",
    "        # Nh√≥m c√°c c·ª•m ƒë·ª©ng tr∆∞·ªõc ƒë·ªông t·ª´ v√†o \"ch·ªß ng·ªØ m·ªü r·ªông\"\n",
    "        if not seen_verb and label in (\"NP\", \"PP\", \"SBAR\"):\n",
    "            subject_nodes.append(c)\n",
    "            continue\n",
    "\n",
    "        # Khi g·∫∑p VP (ho·∫∑c n√∫t b·∫Øt ƒë·∫ßu b·∫±ng V) ‚Üí b·∫Øt ƒë·∫ßu v√πng v·ªã ng·ªØ\n",
    "        if label == \"VP\" or label.startswith(\"V\"):\n",
    "            seen_verb = True\n",
    "            predicate_nodes.append(c)\n",
    "            continue\n",
    "\n",
    "        # C√°c th√†nh ph·∫ßn c√≤n l·∫°i:\n",
    "        # - N·∫øu ƒë√£ th·∫•y ƒë·ªông t·ª´ ‚Üí xem nh∆∞ thu·ªôc v·ªã ng·ªØ\n",
    "        # - N·∫øu ch∆∞a th·∫•y ‚Üí cho v√†o ch·ªß ng·ªØ (tr∆∞·ªùng h·ª£p c·∫•u tr√∫c h∆°i l·ªách)\n",
    "        if seen_verb:\n",
    "            predicate_nodes.append(c)\n",
    "        else:\n",
    "            subject_nodes.append(c)\n",
    "\n",
    "    subject_text = \" \".join(node_text(n) for n in subject_nodes).strip()\n",
    "    predicate_text = \" \".join(node_text(n) for n in predicate_nodes).strip()\n",
    "\n",
    "    return {\n",
    "        \"subject\": subject_text,\n",
    "        \"predicate\": predicate_text,\n",
    "        \"raw_S\": node_text(s_node),\n",
    "    }\n",
    "\n",
    "text = (\n",
    "    \" C√°c B·ªô, ng√†nh, ƒë·ªãa ph∆∞∆°ng ch·ªâ ƒë·∫°o, t·ªï ch·ª©c r√† so√°t, ki·ªÉm tra vi·ªác qu·∫£n l√Ω v√† s·ª≠ d·ª•ng bi√™n ch·∫ø t·∫°i c√°c c∆° quan, t·ªï ch·ª©c, ƒë∆°n v·ªã s·ª± nghi·ªáp c√¥ng l·∫≠p thu·ªôc v√† tr·ª±c thu·ªôc; c√≥ ph∆∞∆°ng √°n b·ªë tr√≠, s·∫Øp x·∫øp bi√™n ch·∫ø s·ª≠ d·ª•ng v∆∞·ª£t so v·ªõi quy ƒë·ªãnh t·∫°i c√°c c∆° quan, t·ªï ch·ª©c, ƒë∆°n v·ªã s∆∞ nghi·ªáp c√¥ng l·∫≠p.\"\n",
    ")\n",
    "\n",
    "doc = nlp(text)\n",
    "\n",
    "for sent in doc.sentences:\n",
    "    tree = sent.constituency\n",
    "    clauses = find_clauses(tree)\n",
    "    for i, s_node in enumerate(clauses, 1):\n",
    "        parts = split_subject_predicate(s_node)\n",
    "        print(f\"Clause {i}:\")\n",
    "        print(\"  Ch·ªß ng·ªØ :\", parts[\"subject\"])\n",
    "        print(\"  V·ªã ng·ªØ  :\", parts[\"predicate\"])\n",
    "        print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
